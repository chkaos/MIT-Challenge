<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>3.Fault Tolerance: primary/backup replication | CHKAOS</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/note/egg.png">
    <meta name="description" content="My Note Collection">
    
    <link rel="preload" href="/note/assets/css/0.styles.ba589cc9.css" as="style"><link rel="preload" href="/note/assets/js/app.d5006f3d.js" as="script"><link rel="preload" href="/note/assets/js/2.65e5b2f3.js" as="script"><link rel="preload" href="/note/assets/js/84.dca226ee.js" as="script"><link rel="prefetch" href="/note/assets/js/10.ae2f2d04.js"><link rel="prefetch" href="/note/assets/js/100.c68d38be.js"><link rel="prefetch" href="/note/assets/js/101.fd3e754f.js"><link rel="prefetch" href="/note/assets/js/102.5c217b08.js"><link rel="prefetch" href="/note/assets/js/103.449bcd79.js"><link rel="prefetch" href="/note/assets/js/104.1b5b375d.js"><link rel="prefetch" href="/note/assets/js/105.a61b2017.js"><link rel="prefetch" href="/note/assets/js/106.e81cfff0.js"><link rel="prefetch" href="/note/assets/js/107.a75923b3.js"><link rel="prefetch" href="/note/assets/js/108.891948b3.js"><link rel="prefetch" href="/note/assets/js/109.a2545875.js"><link rel="prefetch" href="/note/assets/js/11.4ab4a5a3.js"><link rel="prefetch" href="/note/assets/js/110.6d0886e8.js"><link rel="prefetch" href="/note/assets/js/12.255d9150.js"><link rel="prefetch" href="/note/assets/js/13.794e7ac8.js"><link rel="prefetch" href="/note/assets/js/14.d32c7fdd.js"><link rel="prefetch" href="/note/assets/js/15.67cfbf95.js"><link rel="prefetch" href="/note/assets/js/16.3c73a976.js"><link rel="prefetch" href="/note/assets/js/17.8806b699.js"><link rel="prefetch" href="/note/assets/js/18.378bd860.js"><link rel="prefetch" href="/note/assets/js/19.48c8434f.js"><link rel="prefetch" href="/note/assets/js/20.1fdb582e.js"><link rel="prefetch" href="/note/assets/js/21.3e08c9be.js"><link rel="prefetch" href="/note/assets/js/22.810fe1a0.js"><link rel="prefetch" href="/note/assets/js/23.5ba70e2d.js"><link rel="prefetch" href="/note/assets/js/24.9681b927.js"><link rel="prefetch" href="/note/assets/js/25.c0e5deaa.js"><link rel="prefetch" href="/note/assets/js/26.aae4bdc7.js"><link rel="prefetch" href="/note/assets/js/27.7f17e5f5.js"><link rel="prefetch" href="/note/assets/js/28.61ef96a0.js"><link rel="prefetch" href="/note/assets/js/29.f8d63307.js"><link rel="prefetch" href="/note/assets/js/3.031640f4.js"><link rel="prefetch" href="/note/assets/js/30.817d5cd9.js"><link rel="prefetch" href="/note/assets/js/31.e4ea696a.js"><link rel="prefetch" href="/note/assets/js/32.aba63c9c.js"><link rel="prefetch" href="/note/assets/js/33.1aa6facd.js"><link rel="prefetch" href="/note/assets/js/34.69d2bc3f.js"><link rel="prefetch" href="/note/assets/js/35.f82bd55f.js"><link rel="prefetch" href="/note/assets/js/36.20a42b33.js"><link rel="prefetch" href="/note/assets/js/37.7ab6c748.js"><link rel="prefetch" href="/note/assets/js/38.04cf7e1e.js"><link rel="prefetch" href="/note/assets/js/39.87ad9256.js"><link rel="prefetch" href="/note/assets/js/4.0876ef1d.js"><link rel="prefetch" href="/note/assets/js/40.fa12820d.js"><link rel="prefetch" href="/note/assets/js/41.22bd816e.js"><link rel="prefetch" href="/note/assets/js/42.c107915c.js"><link rel="prefetch" href="/note/assets/js/43.983b5fc5.js"><link rel="prefetch" href="/note/assets/js/44.75f6ceb2.js"><link rel="prefetch" href="/note/assets/js/45.98830da6.js"><link rel="prefetch" href="/note/assets/js/46.550f0b22.js"><link rel="prefetch" href="/note/assets/js/47.8fbd223c.js"><link rel="prefetch" href="/note/assets/js/48.edf4b3f9.js"><link rel="prefetch" href="/note/assets/js/49.7508a3de.js"><link rel="prefetch" href="/note/assets/js/5.369c3e08.js"><link rel="prefetch" href="/note/assets/js/50.04715961.js"><link rel="prefetch" href="/note/assets/js/51.277d9998.js"><link rel="prefetch" href="/note/assets/js/52.ebc26274.js"><link rel="prefetch" href="/note/assets/js/53.471c676f.js"><link rel="prefetch" href="/note/assets/js/54.9e46fdf4.js"><link rel="prefetch" href="/note/assets/js/55.8a98c441.js"><link rel="prefetch" href="/note/assets/js/56.b5f6e00a.js"><link rel="prefetch" href="/note/assets/js/57.f6fb0c39.js"><link rel="prefetch" href="/note/assets/js/58.d92a7b7d.js"><link rel="prefetch" href="/note/assets/js/59.33c88c84.js"><link rel="prefetch" href="/note/assets/js/6.34251819.js"><link rel="prefetch" href="/note/assets/js/60.68db27af.js"><link rel="prefetch" href="/note/assets/js/61.ce3eefe1.js"><link rel="prefetch" href="/note/assets/js/62.a3e76ed5.js"><link rel="prefetch" href="/note/assets/js/63.b9084887.js"><link rel="prefetch" href="/note/assets/js/64.e1d868ed.js"><link rel="prefetch" href="/note/assets/js/65.0d77e0a0.js"><link rel="prefetch" href="/note/assets/js/66.fe465e07.js"><link rel="prefetch" href="/note/assets/js/67.76ca73a0.js"><link rel="prefetch" href="/note/assets/js/68.2498dd7c.js"><link rel="prefetch" href="/note/assets/js/69.f03ab615.js"><link rel="prefetch" href="/note/assets/js/7.273a5831.js"><link rel="prefetch" href="/note/assets/js/70.cad85a7e.js"><link rel="prefetch" href="/note/assets/js/71.5dcb8b9a.js"><link rel="prefetch" href="/note/assets/js/72.827102a3.js"><link rel="prefetch" href="/note/assets/js/73.315b5fe4.js"><link rel="prefetch" href="/note/assets/js/74.def8de65.js"><link rel="prefetch" href="/note/assets/js/75.a54fced2.js"><link rel="prefetch" href="/note/assets/js/76.6cae1891.js"><link rel="prefetch" href="/note/assets/js/77.12cb32b9.js"><link rel="prefetch" href="/note/assets/js/78.5994be50.js"><link rel="prefetch" href="/note/assets/js/79.bbc37f01.js"><link rel="prefetch" href="/note/assets/js/8.ba3fa28e.js"><link rel="prefetch" href="/note/assets/js/80.eeb65418.js"><link rel="prefetch" href="/note/assets/js/81.f876286e.js"><link rel="prefetch" href="/note/assets/js/82.a58003e1.js"><link rel="prefetch" href="/note/assets/js/83.13c9c65b.js"><link rel="prefetch" href="/note/assets/js/85.38911042.js"><link rel="prefetch" href="/note/assets/js/86.4f2e3cde.js"><link rel="prefetch" href="/note/assets/js/87.bb576b86.js"><link rel="prefetch" href="/note/assets/js/88.ba3e1a83.js"><link rel="prefetch" href="/note/assets/js/89.cc281bcf.js"><link rel="prefetch" href="/note/assets/js/9.557b7160.js"><link rel="prefetch" href="/note/assets/js/90.b68af197.js"><link rel="prefetch" href="/note/assets/js/91.a0945e77.js"><link rel="prefetch" href="/note/assets/js/92.accc5e5d.js"><link rel="prefetch" href="/note/assets/js/93.682fd046.js"><link rel="prefetch" href="/note/assets/js/94.c47d39d0.js"><link rel="prefetch" href="/note/assets/js/95.6363826c.js"><link rel="prefetch" href="/note/assets/js/96.36edf40e.js"><link rel="prefetch" href="/note/assets/js/97.75b6c735.js"><link rel="prefetch" href="/note/assets/js/98.8b5d3421.js"><link rel="prefetch" href="/note/assets/js/99.e03485fe.js">
    <link rel="stylesheet" href="/note/assets/css/0.styles.ba589cc9.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/note/" class="home-link router-link-active"><img src="/note/favicon.png" alt="CHKAOS" class="logo"> <span class="site-name can-hide">CHKAOS</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/note/MIT/" class="nav-link">
  Mit
</a></div><div class="nav-item"><a href="/note/Leetcode/" class="nav-link">
  Leetcode
</a></div><div class="nav-item"><a href="/note/Interview/" class="nav-link">
  Interview
</a></div><div class="nav-item"><a href="http://chkaos.top" target="_blank" rel="noopener noreferrer" class="nav-link external">
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/chkaos/note" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/note/MIT/" class="nav-link">
  Mit
</a></div><div class="nav-item"><a href="/note/Leetcode/" class="nav-link">
  Leetcode
</a></div><div class="nav-item"><a href="/note/Interview/" class="nav-link">
  Interview
</a></div><div class="nav-item"><a href="http://chkaos.top" target="_blank" rel="noopener noreferrer" class="nav-link external">
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/chkaos/note" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/note/mit/" aria-current="page" class="sidebar-link">前言</a></li><li><a href="/note/mit/schedule.html" class="sidebar-link">计划</a></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>计算机科学与编程导论(python)</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>计算机科学数学</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>电子工程与计算机科学导论1</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>电子工程与计算机科学导论2(交流网络)</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>算法导论</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>软件构造</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>计算机网络导论</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>操作系统</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>分布式系统</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/note/mit/fen-bu-shi-xi-tong/" aria-current="page" class="sidebar-link">6.824 分布式系统</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/01.html" class="sidebar-link">Introduction</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/02.html" class="sidebar-link">2: Infrastructure: RPC and threads</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/03.html" aria-current="page" class="active sidebar-link">3.Fault Tolerance: primary/backup replication</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/04.html" class="sidebar-link">4.Fault Tolerance: FDS Case Study</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/05.html" class="sidebar-link">5: Fault Tolerance:Paxos</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/06.html" class="sidebar-link">6: Fault Tolerance:Raft</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/07.html" class="sidebar-link">7.Guest lecturer: Russ Cox (Google/Go)</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/08.html" class="sidebar-link">8.Case Studies: Replicated File System -- Harp</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/09.html" class="sidebar-link">9. Distributed Computing: Sequential consistency</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/10.html" class="sidebar-link">10.Distributed Computing: Relaxed consistency</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/11.html" class="sidebar-link">11.Disconnected Operation: Version Vectors and File Synchronization</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/12.html" class="sidebar-link">13.Disconnected Operation: Eventual Consistency</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/13.html" class="sidebar-link">13.MapReduce revisited</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/14.html" class="sidebar-link">14.Spark Case Study</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/15.html" class="sidebar-link">15.Guest lecturer: Wilson Hsieh (Google)</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/16.html" class="sidebar-link">16.Scaling Memcache at Facebook</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/17.html" class="sidebar-link">17.Case Studies: Relaxed Consistency-PNUTS</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/18.html" class="sidebar-link">18. Case Studies:Dynamo</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/19.html" class="sidebar-link">19.Distributed systems in the real world (Guest lecturer: Emil Sit)</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/20.html" class="sidebar-link">20.Atomicity: Two-Phase Commit</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/21.html" class="sidebar-link">21.Atomicity: Optimistic Concurrency Control</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/22.html" class="sidebar-link">22.Peer-to-peer: Trackerless Bittorrent and DHTs</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/23.html" class="sidebar-link">23.Peer-to-peer: Bitcoin</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/24.html" class="sidebar-link">24.Project demos</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="_3-fault-tolerance-primary-backup-replication"><a href="#_3-fault-tolerance-primary-backup-replication" class="header-anchor">#</a> 3.Fault Tolerance: primary/backup replication</h1> <p>Today
Replication
Remus case study
Lab 2 introduction</p> <p>Fault tolerance
we'd like a service that continues despite failures!
available: still useable despite [some class of] failures
correct: act just like a single server to clients
very hard!
very useful!</p> <p>Need a failure model: what will we try to cope with?
Independent fail-stop computer failure
Remus further assumes only one failure at a time
Site-wide power failure (and eventual reboot)
(Network partition)
No bugs, no malice</p> <p>Core idea: replication
<em>Two</em> servers (or more)
Each replica keeps state needed for the service
If one replica fails, others can continue</p> <p>Example: fault-tolerant MapReduce master
lab 1 workers are already fault-tolerant, but not master
master is a &quot;single point of failure&quot;
can we have two masters, in case one fails?
[diagram: M1, M2, workers]
state:
worker list
which jobs done
which workers idle
TCP connection state
program counter</p> <p>Big Questions:
What state to replicate?
How does replica get state?
When to cut over to backup?
Are anomalies visible at cut-over?
How to repair / re-integrate?</p> <p>Two main approaches:
State transfer
&quot;Primary&quot; replica executes the service
Primary sends [new] state to backups
Replicated state machine
All replicas execute all operations
If same start state,
same operations,
same order,
deterministic,
then same end state</p> <p>State transfer is simpler
But state may be large, slow to transfer
Remus uses state transfer</p> <p>Replicated state machine can be more efficient
If operations are small compared to data
But complex, e.g. order on multi-core, determinism
Labs use replicated state machines</p> <p>Remus: High Availability via Asynchronous Virtual Machine Replication
NSDI 2008</p> <p>Very ambitious system:
Whole-system replication
Completely transparent to applications and clients
High availability for any existing software
Would be magic if it worked well!
Failure model:
1. independent hardware faults
2. site-wide power failure</p> <p>Plan 1 (slow, broken):
[diagram: app, O/S, Remus underneath]
two machines, primary and backup; plus net and other machines
primary runs o/s and application s/w, talks to clients, &amp;c
backup does <em>not</em> initially execute o/s, applications, &amp;c
it only executes some Remus code
a few times per second:
pause primary
copy entire RAM, registers, disk to backup
resume primary
if primary fails:
start backup executing!</p> <p>Q: is Plan 1 correct?
i.e. does it look just like a single reliable server?</p> <p>Q: what will outside world see if primary fails and replica takes over?
will backup have same state as last visible on primary?
might a client request be lost? executed twice?</p> <p>Q: is Plan 1 efficient?</p> <p>Can we eliminate the fact that backup <em>state</em> trails the primary?
Seems very hard!
Primary would have to tell backup (and wait) on every instruction.</p> <p>Can we <em>conceal</em> the fact that backup's state lags primary?
Prevent outside world from <em>seeing</em> that backup is behind last primary state
e.g. prevent primary sent RPC reply but backup state doesn't reflect that RPC
e.g. MR Register RPC, which it would be bad for backup to forget
Idea: primary &quot;holds&quot; output until backup state catches up to output point
e.g. primary receives RPC request, processes it, creates reply packet,
but Remus holds reply packet until backup has received corresponding state update</p> <p>Remus epochs, checkpoints
Clients:    C1
req1                       reply1
Primary:    ... E1 ... | pause |  E2  release   | pause |
ckpt        ok           ckpt
Backup:      ... (E0) ... |  apply  | (E1)         |</p> <ol><li>Primary runs for a while in Epoch 1, holding E1's output</li> <li>Primary pauses</li> <li>Primary sends RAM+disk changes to backup (in background)</li> <li>Primary resumes execution in E2, holding E2's output</li> <li>Backup copies all to separate RAM, then ACKs</li> <li>Primary releases E1's output</li> <li>Backup applies E1's changes to RAM and disk</li></ol> <p>If primary fails, backup finishes applying last epoch's disk+RAM,
then starts executing</p> <p>Q: any externally visible anomalies?
lost input/output?
repeated output?</p> <p>Q: what if primary receives+executes a request, crashes before checkpoint?
backup won't have seen request!</p> <p>Q: what if primary crashes after sending ckpt to backup,
but before releasing output?</p> <p>Q: what if client doesn't use TCP -- doesn't re-transmit?</p> <p>Q: what if primary fails while sending state to backup?
i.e. backup is mid-way through absorbing new state?</p> <p>Q: are there situations in which Remus will incorrectly activate the backup?
i.e. primary is actually alive
network partition...</p> <p>Q: when primary recovers, how does Remus restore replication?
needed, since eventually active ex-backup will itself fail</p> <p>Q: what if <em>both</em> fail, e.g. site-wide power failure?
RAM content will be lost, but disks will probably survive
after power is restored, reboot guest from one of the disks
O/S and application recovery code will execute
disk must be &quot;crash-consistent&quot;
so probably not the backup disk if was in middle of installing checkpoint
disk shouldn't reflect any held outputs (... why not?)
so probably not the primary's disk if was executing
I do not understand this part of the paper (Section 2.5)
seems to be a window during which neither disk could be used if power failed
primary writes its disk during epoch
meanwhile backup applies last epoch's writes to its disk</p> <p>Q: in what situations will Remus likely have good performance?</p> <p>Q: in what situations will Remus likely have low performance?</p> <p>Q: should epochs be short or long?</p> <p>Remus Evaluation
summary: 1/2 to 1/4 native speed
checkpoints are big and take time to send
output hold limits speed at which clients can interact</p> <p>Why so slow?
checkpoints are big and take time to generate and send
100ms for SPECweb2005 -- because many pages written
so inter-checkpoint intervals must be long
so output must be held for quite a while
so client interactions are slow
only 10 RPCs per second per client</p> <p>How could one get better performance for replication?
big savings possible with application-specific schemes:
just send state really needed by application, not all state
send state in optimized format, not whole pages
send operations if they are smaller than state
likely <em>not</em> transaparent to applications
and probably not to clients either</p> <p>PRIMARY-BACKUP REPLICATION IN LAB 2</p> <p>outline:
simple key/value database
Get(k), Put(k, v), Append(k, v)
primary and backup
replicate by primary sending each operation to backups
tolerate network problems, including partition
either keep going, correctly
or suspend operations until network is repaired
allow replacement of failed servers
you implement essentially all of this (unlike lab 1)</p> <p>&quot;view server&quot; decides who p and b are
main goal: avoid &quot;split brain&quot; -- disagreement about who primary is
clients and servers ask view server
they don't make independent decisions</p> <p>repair:
view server can co-opt &quot;idle&quot; server as b after old b becomes p
primary initializes new backup's state</p> <p>key points:</p> <ol><li>only one primary at a time!</li> <li>the primary must have the latest state!
we will work out some rules to ensure these</li></ol> <p>view server
maintains a sequence of &quot;views&quot;
view #, primary, backup
0: -- --
1: S1 --
2: S1 S2
4: S2 --
3: S2 S3
monitors server liveness
each server periodically sends a Ping RPC
&quot;dead&quot; if missed N Pings in a row
&quot;live&quot; after single Ping
can be more than two servers Pinging view server
if more than two, &quot;idle&quot; servers
if primary is dead
new view with previous backup as primary
if backup is dead, or no backup
new view with previously idle server as backup
OK to have a view with just a primary, and no backup
but -- if an idle server is available, make it the backup</p> <p>how to ensure new primary has up-to-date replica of state?
only promote previous backup
i.e. don't make an idle server the primary
backup must remember if it has been initialized by primary
if not, don't function as primary even if promoted!</p> <p>Q: can more than one server think it is primary?
1: S1, S2
net broken, so viewserver thinks S1 dead but it's alive
2: S2, --
now S1 alive and not aware of view #2, so S1 still thinks it is primary
AND S2 alive and thinks it is primary
=&gt; split brain, no good</p> <p>how to ensure only one server acts as primary?
even though more than one may <em>think</em> it is primary
&quot;acts as&quot; == executes and responds to client requests
the basic idea:
1: S1 S2
2: S2 --
S1 still thinks it is primary
S1 must forward ops to S2
S2 thinks S2 is primary
so S2 must reject S1's forwarded ops</p> <p>the rules:</p> <ol><li>primary in view i must have been primary or backup in view i-1</li> <li>if you think you are primary, must wait for backup for each request</li> <li>if you think you are not backup, reject forwarded requests</li> <li>if you think you are not primary, reject direct client requests</li></ol> <p>so:
before S2 hears about view #2
S1 can process ops from clients, S2 will accept forwarded requests
S2 will reject ops from clients who have heard about view #2
after S2 hears about view #2
if S1 receives client request, it will forward, S2 will reject
so S1 can no longer act as primary
S1 will send error to client, client will ask vs for new view,
client will re-send to S2
the true moment of switch-over occurs when S2 hears about view #2</p> <p>how can new backup get state?
e.g. all the keys and values
if S2 is backup in view i, but was not in view i-1,
S2 should ask primary to transfer the complete state</p> <p>rule for state transfer:
every operation (Put/Get/Append) must be either before or after state xfer
== state xfer must be atomic w.r.t. operations
either
op is before, and xferred state reflects op
op is after, xferred state doesn't reflect op, prim forwards op after state</p> <p>Q: does primary need to forward Get()s to backup?
after all, Get() doesn't change anything, so why does backup need to know?
and the extra RPC costs time</p> <p>Q: how could we make primary-only Get()s work?</p> <p>Q: are there cases when the lab 2 protocol cannot make forward progress?
View service fails
Primary fails before new backup gets state
We will start fixing those in lab 3</p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">lastUpdate:</span> <span class="time">4/6/2021, 2:11:34 PM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/note/mit/fen-bu-shi-xi-tong/02.html" class="prev">
        2: Infrastructure: RPC and threads
      </a></span> <span class="next"><a href="/note/mit/fen-bu-shi-xi-tong/04.html">
        4.Fault Tolerance: FDS Case Study
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/note/assets/js/app.d5006f3d.js" defer></script><script src="/note/assets/js/2.65e5b2f3.js" defer></script><script src="/note/assets/js/84.dca226ee.js" defer></script>
  </body>
</html>
