<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>9. Distributed Computing: Sequential consistency | CHKAOS</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/note/egg.png">
    <meta name="description" content="My Note Collection">
    
    <link rel="preload" href="/note/assets/css/0.styles.ba589cc9.css" as="style"><link rel="preload" href="/note/assets/js/app.d5006f3d.js" as="script"><link rel="preload" href="/note/assets/js/2.65e5b2f3.js" as="script"><link rel="preload" href="/note/assets/js/90.b68af197.js" as="script"><link rel="prefetch" href="/note/assets/js/10.ae2f2d04.js"><link rel="prefetch" href="/note/assets/js/100.c68d38be.js"><link rel="prefetch" href="/note/assets/js/101.fd3e754f.js"><link rel="prefetch" href="/note/assets/js/102.5c217b08.js"><link rel="prefetch" href="/note/assets/js/103.449bcd79.js"><link rel="prefetch" href="/note/assets/js/104.1b5b375d.js"><link rel="prefetch" href="/note/assets/js/105.a61b2017.js"><link rel="prefetch" href="/note/assets/js/106.e81cfff0.js"><link rel="prefetch" href="/note/assets/js/107.a75923b3.js"><link rel="prefetch" href="/note/assets/js/108.891948b3.js"><link rel="prefetch" href="/note/assets/js/109.a2545875.js"><link rel="prefetch" href="/note/assets/js/11.4ab4a5a3.js"><link rel="prefetch" href="/note/assets/js/110.6d0886e8.js"><link rel="prefetch" href="/note/assets/js/12.255d9150.js"><link rel="prefetch" href="/note/assets/js/13.794e7ac8.js"><link rel="prefetch" href="/note/assets/js/14.d32c7fdd.js"><link rel="prefetch" href="/note/assets/js/15.67cfbf95.js"><link rel="prefetch" href="/note/assets/js/16.3c73a976.js"><link rel="prefetch" href="/note/assets/js/17.8806b699.js"><link rel="prefetch" href="/note/assets/js/18.378bd860.js"><link rel="prefetch" href="/note/assets/js/19.48c8434f.js"><link rel="prefetch" href="/note/assets/js/20.1fdb582e.js"><link rel="prefetch" href="/note/assets/js/21.3e08c9be.js"><link rel="prefetch" href="/note/assets/js/22.810fe1a0.js"><link rel="prefetch" href="/note/assets/js/23.5ba70e2d.js"><link rel="prefetch" href="/note/assets/js/24.9681b927.js"><link rel="prefetch" href="/note/assets/js/25.c0e5deaa.js"><link rel="prefetch" href="/note/assets/js/26.aae4bdc7.js"><link rel="prefetch" href="/note/assets/js/27.7f17e5f5.js"><link rel="prefetch" href="/note/assets/js/28.61ef96a0.js"><link rel="prefetch" href="/note/assets/js/29.f8d63307.js"><link rel="prefetch" href="/note/assets/js/3.031640f4.js"><link rel="prefetch" href="/note/assets/js/30.817d5cd9.js"><link rel="prefetch" href="/note/assets/js/31.e4ea696a.js"><link rel="prefetch" href="/note/assets/js/32.aba63c9c.js"><link rel="prefetch" href="/note/assets/js/33.1aa6facd.js"><link rel="prefetch" href="/note/assets/js/34.69d2bc3f.js"><link rel="prefetch" href="/note/assets/js/35.f82bd55f.js"><link rel="prefetch" href="/note/assets/js/36.20a42b33.js"><link rel="prefetch" href="/note/assets/js/37.7ab6c748.js"><link rel="prefetch" href="/note/assets/js/38.04cf7e1e.js"><link rel="prefetch" href="/note/assets/js/39.87ad9256.js"><link rel="prefetch" href="/note/assets/js/4.0876ef1d.js"><link rel="prefetch" href="/note/assets/js/40.fa12820d.js"><link rel="prefetch" href="/note/assets/js/41.22bd816e.js"><link rel="prefetch" href="/note/assets/js/42.c107915c.js"><link rel="prefetch" href="/note/assets/js/43.983b5fc5.js"><link rel="prefetch" href="/note/assets/js/44.75f6ceb2.js"><link rel="prefetch" href="/note/assets/js/45.98830da6.js"><link rel="prefetch" href="/note/assets/js/46.550f0b22.js"><link rel="prefetch" href="/note/assets/js/47.8fbd223c.js"><link rel="prefetch" href="/note/assets/js/48.edf4b3f9.js"><link rel="prefetch" href="/note/assets/js/49.7508a3de.js"><link rel="prefetch" href="/note/assets/js/5.369c3e08.js"><link rel="prefetch" href="/note/assets/js/50.04715961.js"><link rel="prefetch" href="/note/assets/js/51.277d9998.js"><link rel="prefetch" href="/note/assets/js/52.ebc26274.js"><link rel="prefetch" href="/note/assets/js/53.471c676f.js"><link rel="prefetch" href="/note/assets/js/54.9e46fdf4.js"><link rel="prefetch" href="/note/assets/js/55.8a98c441.js"><link rel="prefetch" href="/note/assets/js/56.b5f6e00a.js"><link rel="prefetch" href="/note/assets/js/57.f6fb0c39.js"><link rel="prefetch" href="/note/assets/js/58.d92a7b7d.js"><link rel="prefetch" href="/note/assets/js/59.33c88c84.js"><link rel="prefetch" href="/note/assets/js/6.34251819.js"><link rel="prefetch" href="/note/assets/js/60.68db27af.js"><link rel="prefetch" href="/note/assets/js/61.ce3eefe1.js"><link rel="prefetch" href="/note/assets/js/62.a3e76ed5.js"><link rel="prefetch" href="/note/assets/js/63.b9084887.js"><link rel="prefetch" href="/note/assets/js/64.e1d868ed.js"><link rel="prefetch" href="/note/assets/js/65.0d77e0a0.js"><link rel="prefetch" href="/note/assets/js/66.fe465e07.js"><link rel="prefetch" href="/note/assets/js/67.76ca73a0.js"><link rel="prefetch" href="/note/assets/js/68.2498dd7c.js"><link rel="prefetch" href="/note/assets/js/69.f03ab615.js"><link rel="prefetch" href="/note/assets/js/7.273a5831.js"><link rel="prefetch" href="/note/assets/js/70.cad85a7e.js"><link rel="prefetch" href="/note/assets/js/71.5dcb8b9a.js"><link rel="prefetch" href="/note/assets/js/72.827102a3.js"><link rel="prefetch" href="/note/assets/js/73.315b5fe4.js"><link rel="prefetch" href="/note/assets/js/74.def8de65.js"><link rel="prefetch" href="/note/assets/js/75.a54fced2.js"><link rel="prefetch" href="/note/assets/js/76.6cae1891.js"><link rel="prefetch" href="/note/assets/js/77.12cb32b9.js"><link rel="prefetch" href="/note/assets/js/78.5994be50.js"><link rel="prefetch" href="/note/assets/js/79.bbc37f01.js"><link rel="prefetch" href="/note/assets/js/8.ba3fa28e.js"><link rel="prefetch" href="/note/assets/js/80.eeb65418.js"><link rel="prefetch" href="/note/assets/js/81.f876286e.js"><link rel="prefetch" href="/note/assets/js/82.a58003e1.js"><link rel="prefetch" href="/note/assets/js/83.13c9c65b.js"><link rel="prefetch" href="/note/assets/js/84.dca226ee.js"><link rel="prefetch" href="/note/assets/js/85.38911042.js"><link rel="prefetch" href="/note/assets/js/86.4f2e3cde.js"><link rel="prefetch" href="/note/assets/js/87.bb576b86.js"><link rel="prefetch" href="/note/assets/js/88.ba3e1a83.js"><link rel="prefetch" href="/note/assets/js/89.cc281bcf.js"><link rel="prefetch" href="/note/assets/js/9.557b7160.js"><link rel="prefetch" href="/note/assets/js/91.a0945e77.js"><link rel="prefetch" href="/note/assets/js/92.accc5e5d.js"><link rel="prefetch" href="/note/assets/js/93.682fd046.js"><link rel="prefetch" href="/note/assets/js/94.c47d39d0.js"><link rel="prefetch" href="/note/assets/js/95.6363826c.js"><link rel="prefetch" href="/note/assets/js/96.36edf40e.js"><link rel="prefetch" href="/note/assets/js/97.75b6c735.js"><link rel="prefetch" href="/note/assets/js/98.8b5d3421.js"><link rel="prefetch" href="/note/assets/js/99.e03485fe.js">
    <link rel="stylesheet" href="/note/assets/css/0.styles.ba589cc9.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/note/" class="home-link router-link-active"><img src="/note/favicon.png" alt="CHKAOS" class="logo"> <span class="site-name can-hide">CHKAOS</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/note/MIT/" class="nav-link">
  Mit
</a></div><div class="nav-item"><a href="/note/Leetcode/" class="nav-link">
  Leetcode
</a></div><div class="nav-item"><a href="/note/Interview/" class="nav-link">
  Interview
</a></div><div class="nav-item"><a href="http://chkaos.top" target="_blank" rel="noopener noreferrer" class="nav-link external">
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/chkaos/note" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/note/MIT/" class="nav-link">
  Mit
</a></div><div class="nav-item"><a href="/note/Leetcode/" class="nav-link">
  Leetcode
</a></div><div class="nav-item"><a href="/note/Interview/" class="nav-link">
  Interview
</a></div><div class="nav-item"><a href="http://chkaos.top" target="_blank" rel="noopener noreferrer" class="nav-link external">
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/chkaos/note" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/note/mit/" aria-current="page" class="sidebar-link">前言</a></li><li><a href="/note/mit/schedule.html" class="sidebar-link">计划</a></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>计算机科学与编程导论(python)</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>计算机科学数学</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>电子工程与计算机科学导论1</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>电子工程与计算机科学导论2(交流网络)</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>算法导论</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>软件构造</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>计算机网络导论</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>操作系统</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>分布式系统</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/note/mit/fen-bu-shi-xi-tong/" aria-current="page" class="sidebar-link">6.824 分布式系统</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/01.html" class="sidebar-link">Introduction</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/02.html" class="sidebar-link">2: Infrastructure: RPC and threads</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/03.html" class="sidebar-link">3.Fault Tolerance: primary/backup replication</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/04.html" class="sidebar-link">4.Fault Tolerance: FDS Case Study</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/05.html" class="sidebar-link">5: Fault Tolerance:Paxos</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/06.html" class="sidebar-link">6: Fault Tolerance:Raft</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/07.html" class="sidebar-link">7.Guest lecturer: Russ Cox (Google/Go)</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/08.html" class="sidebar-link">8.Case Studies: Replicated File System -- Harp</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/09.html" aria-current="page" class="active sidebar-link">9. Distributed Computing: Sequential consistency</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/10.html" class="sidebar-link">10.Distributed Computing: Relaxed consistency</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/11.html" class="sidebar-link">11.Disconnected Operation: Version Vectors and File Synchronization</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/12.html" class="sidebar-link">13.Disconnected Operation: Eventual Consistency</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/13.html" class="sidebar-link">13.MapReduce revisited</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/14.html" class="sidebar-link">14.Spark Case Study</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/15.html" class="sidebar-link">15.Guest lecturer: Wilson Hsieh (Google)</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/16.html" class="sidebar-link">16.Scaling Memcache at Facebook</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/17.html" class="sidebar-link">17.Case Studies: Relaxed Consistency-PNUTS</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/18.html" class="sidebar-link">18. Case Studies:Dynamo</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/19.html" class="sidebar-link">19.Distributed systems in the real world (Guest lecturer: Emil Sit)</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/20.html" class="sidebar-link">20.Atomicity: Two-Phase Commit</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/21.html" class="sidebar-link">21.Atomicity: Optimistic Concurrency Control</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/22.html" class="sidebar-link">22.Peer-to-peer: Trackerless Bittorrent and DHTs</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/23.html" class="sidebar-link">23.Peer-to-peer: Bitcoin</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/24.html" class="sidebar-link">24.Project demos</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="_9-distributed-computing-sequential-consistency"><a href="#_9-distributed-computing-sequential-consistency" class="header-anchor">#</a> 9. Distributed Computing: Sequential consistency</h1> <p>New Topic: Distributed computing
The Big Idea: your huge computation on a room full of cheap computers!
An old and difficult goal; many approaches; much progress; still hot.
Other cluster computing papers: TreadMarks, MapReduce, Spark
Sub-topic: sharing framework (RPC? memory? storage? MapReduce?)
Sub-topic: detailed semantics</p> <p>Today's approach: distributed shared memory (DSM)
You all know how to write parallel (threaded) Go programs
Let's farm the threads out to a big cluster of machines!
What's missing? shared memory!</p> <p>DSM plan:
Programmer writes parallel program: threads, shared variables, locks, &amp;c
DSM system farms out threads to a cluster of machines
DSM system creates illusion of single shared memory
[diagram: LAN, machines w/ RAM, MGR]</p> <p>DSM advantages
familiar model -- shared variables, locks, &amp;c
general purpose (compared to e.g. MapReduce)
can use existing apps and libraries written for multiprocessors
lots of machines on a LAN much cheaper than huge multiprocessor</p> <p>But:
machines on a LAN don't actually share memory</p> <p>Approach:
Use hardware's virtual memory protection (r/w vs r/o vs invalid)
General idea illustrated with 2 machines:
Part of the address space starts out on M0
On M1, marked invalid
Part of the address space start out on M1
On M0, marked invalid
A thread of the application on M1 may refer to an address that lives on M0
If thread LD/ST to that &quot;shared&quot; address, M1's hardware will take a page fault
Because page is marked invalid
OS propagates page fault to DSM runtime
DSM runtime can fetch page from M0
DSM on M0, marks page invalid, and sends page to M1
DSM on M1 receives it from M0, copies it to underlying physical memory
DSM on M1 marks the page valid
DSM returns from page fault handler
Hardware retries LD/ST
Runs threaded code w/o modification
e.g. matrix multiply, physical simulation, sort</p> <p>Challenges:
Memory model (does memory act like programmers expect it to act?)
Performance (is it fast?)</p> <p>How could there be any doubt about how memory acts?</p> <p>Example
x and y start out = 0
thread 0:
x = 1
if y == 0:
print yes
thread 1:
y = 1
if x == 0:
print yes</p> <p>Would it be OK if both threads printed yes?
Is that even possible?</p> <p>Could they both print &quot;yes&quot; if this were a Go program?
How could that happen?
Why is that allowed?</p> <p>What is a memory model?
It explains how program reads/writes in different threads interact.
A contract:
It gives the compiler/runtime/hardware some freedom to optimize.
It gives the programmer some guarantees to rely on.
You need one on a multiprocessor (e.g. for the labs).
You <em>really</em> need one for a DSM system.
There are many memory models!
With different optimization/convenience trade-offs.
Often called a consistency model.
Needed for any memory-like or storage system (e.g. the labs).</p> <p>What does Go's memory model say?
It answers questions about whether, when and in what order reads and
writes by different threads interact.
It answers questions like:
If a thread says x=1;y=2, could other threads see y=2 but x=0?
If a thread says x=1;tmp=y, must other threads see x=1 after read completes?
By default: Go makes NO GUARANTEE about when/whether/order in which one thread's
memory references interact with other threads.
A write only becomes visible if writer then interacts with reader
via a &quot;synchronization event&quot;, and then the reader reads.
E.g. write releases a lock, reader acquires the lock.</p> <p>So this Go code (from Lab tests!) is not guaranteed to do what I might think:
done := false
go func() {
while done == false {
...
}
}
...
done = true</p> <p>The memory model says the thread may never see the change to done.
Though in practice, with current compiler, this seems to work.
Solutions: lock around ALL USES of done; or (better) a channel.</p> <p>Why does Go break my perfectly intuitive and straightforward code?
Because an optimizing compiler could thereby generate better code.
Model allows any optimization that keeps an individual thread's results the same:
change the order of statements, including variable reads and writes.
perhaps start a slow read early.
put a variable in a register, so changes aren't visible to other cores.
totally eliminate code or variables (done=true is useless!)
evaluate at compile-time (done is always false!)
And because some CPU hardware re-orders reads/writes.
E.g. 2nd write may be visible before 1st if 1st must fetch line from RAM.
E.g. example's load of y may happen before x=1.</p> <p>Back to DSM.</p> <p>Naive distributed shared memory
[diagram]
M0, M1, M2, LAN
each machine has a local copy of all of memory
read: from local memory
write: send update msg to each other host (but don't wait)
fast: never waits for communication</p> <p>Does this naive DSM work well?
What will it do with the example?
This naive DSM is fast but not programmer-friendly.</p> <p>The paper's memory model: sequential consistency
Formal version of &quot;a read sees the most recent write&quot;.
This is a relatively strict and programmer-friendly memory model.
&quot;Most recent&quot; is only meaningful if there's an overall order.</p> <p>Sequential consistency's definition:
The result of any execution must be the same as if
1. the operations of all the processors executed in some total order,
2. each processor's operations appear in the total order in program order,
3. all operations see results consistent with the total order
i.e. read sees most recent write in the order</p> <p>Would sequential consistency cause our example to get the intuitive result?
M0: Wx1 Ry?
M1: Wy1 Rx?
The system must get the same results as if it had merged these into one order,
maintaining the order of each machine's operations.
A few possibilities:
Wx1 Ry0 Wy1 Rx1
Wx1 Wy1 Ry1 Rx1
Wx1 Wy1 Rx1 Ry1
and some more symmetric ones (swap M0 and M1)
the second read (either x or y) is always 1, as you'd hope
What is forbidden?
Wx1 Ry0 Wy1 Rx0 -- read didn't see preceding write (naive system did this)
Ry0 Wy1 Rx0 Wx1 -- M0's instructions out of order (some CPUs do this)</p> <p>This example is a good diagnostic for sequential consistency.</p> <p>Sequential consistency performance is ok but not great
E.g. system must communicate M0's x=1 to M1 before M0 can proceed to read y
To ensure results consistent with a single total order
(Second &quot;forbidden&quot; example)
This communication takes time!</p> <p>A simple implementation of sequential consistency
[diagram]
single memory server
each machine sends r/w ops to server, in order, waiting for reply
server picks order among waiting ops
server executes one by one, sending replies</p> <p>This simple implementation will be slow
single server will get overloaded
no local cache, so all operations wait for server</p> <p>Which brings us to IVY
IVY = Integrated  shared  Virtual  memory  at Yale
Memory Coherence in Shared Virtual Memory Systems, Li and Hudak, PODC 1986</p> <p>IVY big picture
[diagram: M0 w/ a few pages of mem, M1 w/ a few pages, LAN]
Operates on pages of memory, stored in machine DRAM (no mem server)
Each page present in each machine's virtual address space
On each a machine, a page might be invalid, read-only, or read-write
Uses VM hardware to intercept reads/writes</p> <p>Invariant: a page is either:
Read/write on one machine, invalid on all others; or
Read/only on &gt;= 1 machines, read/write on none</p> <p>Basic machinery:
Read fault on an invalid page:
Demote R/W (if any) to R/O
Copy page
Mark local copy R/O
Write fault on an R/O page:
Invalidate all copies
Mark local copy R/W</p> <p>IVY allows multiple reader copies between writes
For speed -- local reads are fast
No need to force an order for reads that occur between two writes
Let them occur concurrently -- a copy of the page at each reader</p> <p>Why crucial to invalidate all copies before write?
Once a write completes, all subsequent reads <em>must</em> see new data
Otherwise we break our example, and don't get sequential consistency</p> <p>How does IVY do on the example?
I.e. could both M0 and M1 print &quot;yes&quot;?
If M0 sees y == 0,
M1 hasn't done it's write to y (no stale data == reads see prior writes),
M1 hasn't read x (each machine in order),
M1 must see x==1 (no stale data == reads see prior writes).</p> <p>Message types:
[don't list these on board, just for reference]
RQ read query (reader to MGR)
RF read forward (MGR to owner)
RD read data (owner to reader)
RC read confirm (reader to MGR)
&amp;c</p> <p>(see ivy-code.txt on web site)</p> <p>scenario 1: M0 has writeable copy (is owner), M1 wants to read
[time diagram: MGR 0 1]
0. page fault on M1, since page must have been marked invalid</p> <ol><li>M1 sends RQ to MGR</li> <li>MGR sends RF to M0, MGR adds M1 to copy_set</li> <li>M0 marks page as access=read, sends RD to M1</li> <li>M1 marks access=read, sends RC to MGR</li></ol> <p>scenario 2: now M2 wants to write
[time diagram: MGR 0 1 2]
0. page fault on M2</p> <ol><li>M2 sends WQ to MGR</li> <li>MGR sends IV to copy_set (i.e. M1)</li> <li>M1 sends IC msg to MGR</li> <li>MGR sends WF to M0, sets owner=M2, copy_set={}</li> <li>M0 sends WD to M2, access=none</li> <li>M2 marks r/w, sends WC to MGR</li></ol> <p>what if two machines want to write the same page at the same time?</p> <p>what if one machine reads just as ownership is changing hands?</p> <p>If M0 issues x=1 at time=10, and M1 issues x=2 at time=20,
is the resulting x value always x?
No: messages can be slow, MGR processes them in arbitrary order.
You cannot use wall-clock time to reason about sequential consistency.</p> <p>what if there were no IC message?
(this is The Question)
i.e. MGR didn't wait for holders of copies to ack?</p> <p>what if there were no WC message?
e.g. MGR unlocked after sending WF to M0?
MGR would send subsequent RF, WF to M2 (new owner)
What if such a WF/RF arrived at M2 before WD?
No problem! M2 has ptable[p].lock locked until it gets WD
RC + info[p].lock prevents RF from being overtaken by a WF
so it's not clear why WC is needed!
but I am not confident in this conclusion</p> <p>In what situations will IVY perform well?</p> <ol><li>Page read by many machines, written by none</li> <li>Page written by just one machine at a time, not used at all by others
Cool that IVY moves pages around in response to changing use patterns</li></ol> <p>Will page size of e.g. 4096 bytes be good or bad?
good if spatial locality, i.e. program looks at large blocks of data
bad if program writes just a few bytes in a page
subsequent readers copy whole page just to get a few new bytes
bad if false sharing
i.e. two unrelated variables on the same page
and at least one is frequently written
page will bounce between different machines
even read-only users of a non-changing variable will get invalidations
even though those computers never use the same location</p> <p>What about IVY's performance?
after all, the point was speedup via parallelism</p> <p>What's the best we could hope for in terms of performance?
Nx faster on N machines</p> <p>What might prevent us from getting Nx speedup?
Application is inherently non-scalable
Can't be split into parallel activities
Application communicates too many bytes
So network prevents more machines yielding more performance
Too many small reads/writes to shared pages
Even if # bytes is small, IVY makes this expensive</p> <p>How well do they do?
Figure 4: near-linear for PDE
Figure 6: very sub-linear for sort
Figure 7: near-linear for matrix multiply</p> <p>Why did sort do poorly?
&quot;block odd-even merge-split&quot;
Here's my guess
N machines, data in 2<em>N partitions
Phase 1: Local sort of 2</em>N partitions for N machines
Phase 2: 2N-1 merge-splits; each round sends all data over network
Phase 1 probably gets linear speedup
Phase 2 probably does not -- limited by LAN speed
also more machines may mean more rounds
So for small # machines, local sort dominates, more machines helps
For large # machines, communication dominates, more machines don't help
Also, more machines shifts from n*log(n) local sort to n^2 bubble-ish short</p> <p>How could one speed up IVY?
next lecture: relax the consistency model
allow multiple writers to same page!</p> <hr> <p>Paper intro says DSM subsumes RPC -- is that true?
When would DSM be better than RPC?
More transparent. Easier to program.
When would RPC be better?
Isolation. Control over communication. Tolerate latency.
Portability. Define your own semantics.
Might you still want RPC in your DSM system? For efficient sleep/wakeup?</p> <p>Known problems in Section 3.1 pseudo-code
Fault handlers must wait for owner to send p before confirming to manager
Deadlock if owner has page r/o and takes write fault
Worrisome that no clear order ptable[p].lock vs info[p].lock
Write server / manager must set owner=request_node
Manager parts of fault handlers don't ask owner for the page
Does processing of the invalidate request hold ptable[p].lock?
probably can't -- deadlock</p> <p>Real-world uses of distributed shared memory?
(these are from searching the web, not direct knowledge)
https://numascale.com/numa_technology.html
http://hazelcast.com/products/hazelcast/
http://www.dell.com/downloads/global/power/ps1q08-50080247-Intel.pdf
http://www.scalemp.com/solutions/shared-memory/</p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">lastUpdate:</span> <span class="time">4/6/2021, 2:11:34 PM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/note/mit/fen-bu-shi-xi-tong/08.html" class="prev">
        8.Case Studies: Replicated File System -- Harp
      </a></span> <span class="next"><a href="/note/mit/fen-bu-shi-xi-tong/10.html">
        10.Distributed Computing: Relaxed consistency
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/note/assets/js/app.d5006f3d.js" defer></script><script src="/note/assets/js/2.65e5b2f3.js" defer></script><script src="/note/assets/js/90.b68af197.js" defer></script>
  </body>
</html>
