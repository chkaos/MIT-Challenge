<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>8.Case Studies: Replicated File System -- Harp | CHKAOS</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/note/egg.png">
    <meta name="description" content="My Note Collection">
    
    <link rel="preload" href="/note/assets/css/0.styles.ba589cc9.css" as="style"><link rel="preload" href="/note/assets/js/app.d5006f3d.js" as="script"><link rel="preload" href="/note/assets/js/2.65e5b2f3.js" as="script"><link rel="preload" href="/note/assets/js/89.cc281bcf.js" as="script"><link rel="prefetch" href="/note/assets/js/10.ae2f2d04.js"><link rel="prefetch" href="/note/assets/js/100.c68d38be.js"><link rel="prefetch" href="/note/assets/js/101.fd3e754f.js"><link rel="prefetch" href="/note/assets/js/102.5c217b08.js"><link rel="prefetch" href="/note/assets/js/103.449bcd79.js"><link rel="prefetch" href="/note/assets/js/104.1b5b375d.js"><link rel="prefetch" href="/note/assets/js/105.a61b2017.js"><link rel="prefetch" href="/note/assets/js/106.e81cfff0.js"><link rel="prefetch" href="/note/assets/js/107.a75923b3.js"><link rel="prefetch" href="/note/assets/js/108.891948b3.js"><link rel="prefetch" href="/note/assets/js/109.a2545875.js"><link rel="prefetch" href="/note/assets/js/11.4ab4a5a3.js"><link rel="prefetch" href="/note/assets/js/110.6d0886e8.js"><link rel="prefetch" href="/note/assets/js/12.255d9150.js"><link rel="prefetch" href="/note/assets/js/13.794e7ac8.js"><link rel="prefetch" href="/note/assets/js/14.d32c7fdd.js"><link rel="prefetch" href="/note/assets/js/15.67cfbf95.js"><link rel="prefetch" href="/note/assets/js/16.3c73a976.js"><link rel="prefetch" href="/note/assets/js/17.8806b699.js"><link rel="prefetch" href="/note/assets/js/18.378bd860.js"><link rel="prefetch" href="/note/assets/js/19.48c8434f.js"><link rel="prefetch" href="/note/assets/js/20.1fdb582e.js"><link rel="prefetch" href="/note/assets/js/21.3e08c9be.js"><link rel="prefetch" href="/note/assets/js/22.810fe1a0.js"><link rel="prefetch" href="/note/assets/js/23.5ba70e2d.js"><link rel="prefetch" href="/note/assets/js/24.9681b927.js"><link rel="prefetch" href="/note/assets/js/25.c0e5deaa.js"><link rel="prefetch" href="/note/assets/js/26.aae4bdc7.js"><link rel="prefetch" href="/note/assets/js/27.7f17e5f5.js"><link rel="prefetch" href="/note/assets/js/28.61ef96a0.js"><link rel="prefetch" href="/note/assets/js/29.f8d63307.js"><link rel="prefetch" href="/note/assets/js/3.031640f4.js"><link rel="prefetch" href="/note/assets/js/30.817d5cd9.js"><link rel="prefetch" href="/note/assets/js/31.e4ea696a.js"><link rel="prefetch" href="/note/assets/js/32.aba63c9c.js"><link rel="prefetch" href="/note/assets/js/33.1aa6facd.js"><link rel="prefetch" href="/note/assets/js/34.69d2bc3f.js"><link rel="prefetch" href="/note/assets/js/35.f82bd55f.js"><link rel="prefetch" href="/note/assets/js/36.20a42b33.js"><link rel="prefetch" href="/note/assets/js/37.7ab6c748.js"><link rel="prefetch" href="/note/assets/js/38.04cf7e1e.js"><link rel="prefetch" href="/note/assets/js/39.87ad9256.js"><link rel="prefetch" href="/note/assets/js/4.0876ef1d.js"><link rel="prefetch" href="/note/assets/js/40.fa12820d.js"><link rel="prefetch" href="/note/assets/js/41.22bd816e.js"><link rel="prefetch" href="/note/assets/js/42.c107915c.js"><link rel="prefetch" href="/note/assets/js/43.983b5fc5.js"><link rel="prefetch" href="/note/assets/js/44.75f6ceb2.js"><link rel="prefetch" href="/note/assets/js/45.98830da6.js"><link rel="prefetch" href="/note/assets/js/46.550f0b22.js"><link rel="prefetch" href="/note/assets/js/47.8fbd223c.js"><link rel="prefetch" href="/note/assets/js/48.edf4b3f9.js"><link rel="prefetch" href="/note/assets/js/49.7508a3de.js"><link rel="prefetch" href="/note/assets/js/5.369c3e08.js"><link rel="prefetch" href="/note/assets/js/50.04715961.js"><link rel="prefetch" href="/note/assets/js/51.277d9998.js"><link rel="prefetch" href="/note/assets/js/52.ebc26274.js"><link rel="prefetch" href="/note/assets/js/53.471c676f.js"><link rel="prefetch" href="/note/assets/js/54.9e46fdf4.js"><link rel="prefetch" href="/note/assets/js/55.8a98c441.js"><link rel="prefetch" href="/note/assets/js/56.b5f6e00a.js"><link rel="prefetch" href="/note/assets/js/57.f6fb0c39.js"><link rel="prefetch" href="/note/assets/js/58.d92a7b7d.js"><link rel="prefetch" href="/note/assets/js/59.33c88c84.js"><link rel="prefetch" href="/note/assets/js/6.34251819.js"><link rel="prefetch" href="/note/assets/js/60.68db27af.js"><link rel="prefetch" href="/note/assets/js/61.ce3eefe1.js"><link rel="prefetch" href="/note/assets/js/62.a3e76ed5.js"><link rel="prefetch" href="/note/assets/js/63.b9084887.js"><link rel="prefetch" href="/note/assets/js/64.e1d868ed.js"><link rel="prefetch" href="/note/assets/js/65.0d77e0a0.js"><link rel="prefetch" href="/note/assets/js/66.fe465e07.js"><link rel="prefetch" href="/note/assets/js/67.76ca73a0.js"><link rel="prefetch" href="/note/assets/js/68.2498dd7c.js"><link rel="prefetch" href="/note/assets/js/69.f03ab615.js"><link rel="prefetch" href="/note/assets/js/7.273a5831.js"><link rel="prefetch" href="/note/assets/js/70.cad85a7e.js"><link rel="prefetch" href="/note/assets/js/71.5dcb8b9a.js"><link rel="prefetch" href="/note/assets/js/72.827102a3.js"><link rel="prefetch" href="/note/assets/js/73.315b5fe4.js"><link rel="prefetch" href="/note/assets/js/74.def8de65.js"><link rel="prefetch" href="/note/assets/js/75.a54fced2.js"><link rel="prefetch" href="/note/assets/js/76.6cae1891.js"><link rel="prefetch" href="/note/assets/js/77.12cb32b9.js"><link rel="prefetch" href="/note/assets/js/78.5994be50.js"><link rel="prefetch" href="/note/assets/js/79.bbc37f01.js"><link rel="prefetch" href="/note/assets/js/8.ba3fa28e.js"><link rel="prefetch" href="/note/assets/js/80.eeb65418.js"><link rel="prefetch" href="/note/assets/js/81.f876286e.js"><link rel="prefetch" href="/note/assets/js/82.a58003e1.js"><link rel="prefetch" href="/note/assets/js/83.13c9c65b.js"><link rel="prefetch" href="/note/assets/js/84.dca226ee.js"><link rel="prefetch" href="/note/assets/js/85.38911042.js"><link rel="prefetch" href="/note/assets/js/86.4f2e3cde.js"><link rel="prefetch" href="/note/assets/js/87.bb576b86.js"><link rel="prefetch" href="/note/assets/js/88.ba3e1a83.js"><link rel="prefetch" href="/note/assets/js/9.557b7160.js"><link rel="prefetch" href="/note/assets/js/90.b68af197.js"><link rel="prefetch" href="/note/assets/js/91.a0945e77.js"><link rel="prefetch" href="/note/assets/js/92.accc5e5d.js"><link rel="prefetch" href="/note/assets/js/93.682fd046.js"><link rel="prefetch" href="/note/assets/js/94.c47d39d0.js"><link rel="prefetch" href="/note/assets/js/95.6363826c.js"><link rel="prefetch" href="/note/assets/js/96.36edf40e.js"><link rel="prefetch" href="/note/assets/js/97.75b6c735.js"><link rel="prefetch" href="/note/assets/js/98.8b5d3421.js"><link rel="prefetch" href="/note/assets/js/99.e03485fe.js">
    <link rel="stylesheet" href="/note/assets/css/0.styles.ba589cc9.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/note/" class="home-link router-link-active"><img src="/note/favicon.png" alt="CHKAOS" class="logo"> <span class="site-name can-hide">CHKAOS</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/note/MIT/" class="nav-link">
  Mit
</a></div><div class="nav-item"><a href="/note/Leetcode/" class="nav-link">
  Leetcode
</a></div><div class="nav-item"><a href="/note/Interview/" class="nav-link">
  Interview
</a></div><div class="nav-item"><a href="http://chkaos.top" target="_blank" rel="noopener noreferrer" class="nav-link external">
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/chkaos/note" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/note/MIT/" class="nav-link">
  Mit
</a></div><div class="nav-item"><a href="/note/Leetcode/" class="nav-link">
  Leetcode
</a></div><div class="nav-item"><a href="/note/Interview/" class="nav-link">
  Interview
</a></div><div class="nav-item"><a href="http://chkaos.top" target="_blank" rel="noopener noreferrer" class="nav-link external">
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/chkaos/note" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/note/mit/" aria-current="page" class="sidebar-link">前言</a></li><li><a href="/note/mit/schedule.html" class="sidebar-link">计划</a></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>计算机科学与编程导论(python)</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>计算机科学数学</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>电子工程与计算机科学导论1</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>电子工程与计算机科学导论2(交流网络)</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>算法导论</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>软件构造</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>计算机网络导论</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>操作系统</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>分布式系统</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/note/mit/fen-bu-shi-xi-tong/" aria-current="page" class="sidebar-link">6.824 分布式系统</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/01.html" class="sidebar-link">Introduction</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/02.html" class="sidebar-link">2: Infrastructure: RPC and threads</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/03.html" class="sidebar-link">3.Fault Tolerance: primary/backup replication</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/04.html" class="sidebar-link">4.Fault Tolerance: FDS Case Study</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/05.html" class="sidebar-link">5: Fault Tolerance:Paxos</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/06.html" class="sidebar-link">6: Fault Tolerance:Raft</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/07.html" class="sidebar-link">7.Guest lecturer: Russ Cox (Google/Go)</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/08.html" aria-current="page" class="active sidebar-link">8.Case Studies: Replicated File System -- Harp</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/09.html" class="sidebar-link">9. Distributed Computing: Sequential consistency</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/10.html" class="sidebar-link">10.Distributed Computing: Relaxed consistency</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/11.html" class="sidebar-link">11.Disconnected Operation: Version Vectors and File Synchronization</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/12.html" class="sidebar-link">13.Disconnected Operation: Eventual Consistency</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/13.html" class="sidebar-link">13.MapReduce revisited</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/14.html" class="sidebar-link">14.Spark Case Study</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/15.html" class="sidebar-link">15.Guest lecturer: Wilson Hsieh (Google)</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/16.html" class="sidebar-link">16.Scaling Memcache at Facebook</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/17.html" class="sidebar-link">17.Case Studies: Relaxed Consistency-PNUTS</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/18.html" class="sidebar-link">18. Case Studies:Dynamo</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/19.html" class="sidebar-link">19.Distributed systems in the real world (Guest lecturer: Emil Sit)</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/20.html" class="sidebar-link">20.Atomicity: Two-Phase Commit</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/21.html" class="sidebar-link">21.Atomicity: Optimistic Concurrency Control</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/22.html" class="sidebar-link">22.Peer-to-peer: Trackerless Bittorrent and DHTs</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/23.html" class="sidebar-link">23.Peer-to-peer: Bitcoin</a></li><li><a href="/note/mit/fen-bu-shi-xi-tong/24.html" class="sidebar-link">24.Project demos</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="_8-case-studies-replicated-file-system-harp"><a href="#_8-case-studies-replicated-file-system-harp" class="header-anchor">#</a> 8.Case Studies: Replicated File System -- Harp</h1> <p>Replication in the Harp File System
Liskov, Ghemawat, Gruber, Johnson, Shrira, Williams
SOSP 1991</p> <p>Why are we reading this paper?
Harp was the first complete replication system that dealt w/ partition
It's a complete case study of a replicated service (file server)
It uses Raft-like replication techniques</p> <p>How could a 1991 paper still be worth reading?
Harp introduced techniques that are still widely used
There are few papers describing complete replicated systems</p> <p>The paper is a mix of fundamentals and incidentals
We care a lot about replication
We may not care much about NFS specifically
But we care a lot about the challenges faced when integrating
a real application with a replication protocol.
And we care about where optimization is possible.</p> <p>I'm going to focus on parts of Harp that aren't already present in Raft.
But note that Harp pre-dates Raft by 20+ years.
Raft is, to a great extent, a tutorial on ideas pioneered by Harp.
Though they differ in many details.</p> <p>What does Harp paper explain that Raft paper does not?
Adapting a complex service to state machine abstraction
e.g. possibility of applying an operation twice
Lots of optimizations
pipelining of requests to backup
witness
primary-only execution of read-only operations using leases
Efficient re-integration of re-started server with large state
Power failure, including simultaneous failure of all servers
Efficient persistence on disk</p> <p>Harp authors had not implemented recovery yet
Earlier paper (1988) describes View-stamped Replication:
http://www.pmg.csail.mit.edu/papers/vr.pdf
Later (2006) paper has clearer description, though a bit different:
http://pmg.csail.mit.edu/papers/vr-revisited.pdf</p> <p>Basic setup is familiar
Clients, primary, backup(s), witness(es).
Client -&gt; Primary
Primary -&gt; Backups
Backups -&gt; Primary
Primary waits for all backups / promoted witnesses in current view
Commit point
Primary -&gt; execute and reply to Client
Primary -&gt; tell Backups to Commit</p> <p>Why are 2b+1 servers necessary to tolerate b failures?
Requiring majority for each operation helps ensure no other partition.
And majority intersection helps ensure state persists to future views.
Not waiting for b allows tolerance of b failures.</p> <p>What are Harp's witnesses?
The witnesses are one significant difference from Raft.
The b witnesses do not ordinarily hear about operations or keep state.
Why is that OK?
b+1 of 2b+1 do have state
So any b failures leaves at least one live copy of state.
Why are the b witnesses needed at all?
If b replicas with state do fail, witnesses give the required b+1 majority.
To ensure that only one partition operates -- no split brain.
So, for a 3-server system, the witness is there to break ties about
which partition is allowed to operate when primary and backup are
in different partitions. The partition with the witness wins.</p> <p>Does primary need to send operations to witnesses?
The primary must collect ACKs from a majority of the 2b+1 for every r/w operation.
To ensure that it is still the primary -- still in the majority partition.
To ensure that operation is on enough servers to intersect with any
future majority that forms a new view.
If all backups are up, primary+backups are enough for that majority.
If m backups are down:
Primary must talk to m &quot;promoted&quot; witnesses to get its majority for each op.
Those witnesses must record the op, to ensure overlap with any
future majority.
Thus each &quot;promoted&quot; witness keeps a log.</p> <p>So in a 2b+1 system, a view always has b+1 servers that the primary
must contact for each op, and that store each op.</p> <p>Note: somewhat different from Raft
Raft keeps sending each op to all servers, proceeds when majority answer
So leader must keep full log until failed server re-joins
Harp eliminates failed server from view, doesn't send to it
Only witness has to keep a big log; has special plan (ram, disk, tape).
The bigger issue is that it can take a lot of work to bring
a re-joining replica up to date; careful design is required.</p> <p>What's the story about the UPS?
This is one of the most interesting aspects of Harp's design
Each server's power cord is  plugged into a UPS
UPS has enough battery to run server for a few minutes
UPS tells server (via serial port) when main A/C power fails
Server writes dirty FS blocks and Harp log to disk, then shuts down</p> <p>What does the UPS buy for Harp?
Efficient protection against A/C power failure of ALL servers
For failures of up to b servers, replication is enough
If <em>all</em> servers failed and lost state, that's more than b failures,
so Harp has no guarantee (and indeed no state!)
With UPS:
Each server can reply <em>without</em> writing disk!
But still guarantees to retain latest state despite simultaneous power fail
But note:
UPS does not protect against other causes of simultaneous failure
e.g. bugs, earthquake
Harp treats servers that re-start after UPS-protected crash differently
than those that re-start with crash that lost in-memory state
Because the latter may have forgotten <em>committed</em> operations</p> <p>Larger point, faced by every fault-tolerant system
Replicas <em>must</em> keep persistent state to deal w/ failure of all servers
Committed operations
Latest view number, proposal number, &amp;c
Must persist this state before replying
Writing every commit to disk is very slow!
10 ms per disk write, so only 100 ops/second
So there are a few common patterns:
1. Low throughput
2. Batching, high delay
3. Lossy or inconsistent recovery from simultaneous failure
4. Batteries, flash, SSD w/ capacitor, &amp;c</p> <p>Let's talk about Harp's log management and operation execution
Primary and backup must apply client operations to their state
State here is a file system -- directories, file, owners, permissions, &amp;c
Harp must mimic an ordinary NFS server to the client
i.e. not forget about ops for which it has sent a reply</p> <p>What is in a typical log record?
Client's NFS operation (write, mkdir, chmod, &amp;c)
Shadow state: modified i-nodes and directory content <em>after</em> execution
Client RPC request ID, for duplicate detection
Reply to send to client, for duplicate detection</p> <p>Why does Harp have so many log pointers?
FP most recent client request
CP commit point (real in primary, latest heard in backup)
AP highest update sent to disk
LB disk has finished writing up to here
GLB all nodes have completed disk up to here</p> <p>Why the FP-CP gap?
So primary doesn't need to wait for ACKs from each backup
before sending next operation to backups
Primary pipelines ops CP..FP to the backups.
Higher throughput if concurrent client requests.</p> <p>Why the AP-LB gap?
Allows Harp to issue many ops as disk writes before waiting for disk
The disk is more efficient if it has lots of writes (e.g. arm scheduling)</p> <p>What is the LB?
This replica has everything &lt;= LB on disk.
So it won't need those log records again.</p> <p>Why the LB-GLB gap?
GLB is min(all servers' LBs).
GLB is earliest record that <em>some</em> server might need if it loses memory.</p> <p>When does Harp execute a client operation?
There are two answers!</p> <ol><li>When operation arrives, primary figures out exactly what should happen.
Produces resulting on-disk bytes for modified i-nodes, directories, &amp;c.
This is the shadow state.
This happens before the CP, so the primary must consult recent
operations in the log to find the latest file system state.</li> <li>After the operation commits, primary and backup can apply it to
their file systems.
They copy the log entry's shadow state to the file system;
they do not really execute the operation.
And now the primary can reply to the client's RPC.</li></ol> <p>Why does Harp split execution in this way?
If a server crashes and reboots, it is brought up to date by replaying
log entries it might have missed. Harp can't know exactly what the
last pre-crash operation was, so Harp may repeat some. It's not
correct to fully execute some operations twice, e.g. file append.
So Harp log entries contain the <em>resulting</em> state, which is what's applied.</p> <p>The point: multiple replay means replication isn't transparent to the service.
Service must be modified to generate and accept the state
modifications that result from client operations.
In general, when applying replication to existing services,
the service must be modified to cope with multiple replay.</p> <p>Can Harp primary execute read-only operations w/o replicating to backups?
e.g. reading a file.
Would be faster -- after all, there's no new data to replicate.
What's the danger?
Harp's idea: leases
Backups promise not to form a new view for some time.
Primary can execute read-only ops locally for that time minus slop.
Depends on reasonably synchronized clocks:
Primary and backup must have bounded disagreement
on how fast time passes.</p> <p>What state should primary use when executing a read-only operation?
Does it have to wait for all previously arrived operations to commit?
No! That would be almost as slow as committing the read-only op.
Should it look at state as of operation at FP, i.e. latest r/w operation?
No! That operation has not committed; not allowed to reveal its effects.
Thus Harp executes read-only ops with state as of CP.
What if client sends a WRITE and (before WRITE finishes) a READ of same data?
READ may see data <em>before</em> the WRITE!
Why is that OK?</p> <p>How does failure recovery work?
I.e. how does Harp recover replicated state during view change?</p> <p>Setup for the following scenarios
5 servers: 1 is usually primary, 2+3 are backups, 4+5 witnesses</p> <p>Scenario:
S1+S2+S3; then S1 crashes
S2 is primary in new view (and S4 is promoted)
Will S2 have every committed operation?
Will S2 have every operation S1 received?
Will S2's log tail be the same as S3's log tail?
How far back can S2 and S3 log tail differ?
How to cause S2 and S3's log to be the same?
Must commit ops that appeared in both S2+S3 logs
What about ops that appear in only one log?
In this scenario, can discard since could not have committed
But in general committed op might be visible in just one log
From what point does promoted witness have to start keeping a log?</p> <p>What if S1 crashed just before replying to a client?
Will the client ever get a reply?</p> <p>After S1 recovers, with intact disk, but lost memory.
It will be primary, but Harp can't immediately use its state or log.
Unlike Raft, where leader only elected if it has the best log.
Harp must replay log from promoted witness (S4)
Could S1 have executed an op just before crashing
that the replicas didn't execute after taking over?
No, execution up to CP only, and CP is safe on S2+S3.</p> <p>New scenario: S2 and S3 are partitioned (but still alive)
Can S1+S4+S5 continue to process operations?
Yes, promoted witnesses S4+S5
S4 moves to S2/S3 partition
Can S1+S5 continue?
No, primary S1 doesn't get enough backup ACKs
Can S2+S3+S4 continue?
Yes, new view copies log entries S4-&gt;S2, S4-&gt;S3, now S2 is primary
Note:
New primary was missing many committed operations
In general some <em>committed</em> operations may be on only one server</p> <p>New scenario: S2 and S3 are partitioned (but still alive)
S4 crashes, loses memory contents, reboots in S2/S3 partition
Can they continue?
Only if there wasn't another view that formed and committed more ops
How to detect?
Depends on what S4's on-disk view # says.
OK if S4's disk view # is same as S2+S3's.
No new views formed.
S2+S3 must have heard about all committed ops in old view.</p> <p>Everybody suffers a power failure.
S4 disk and memory are lost, but it does re-start after repair.
S1 and S5 never recover.
S2 and S3 save everything on disk, re-start just fine.
Can S2+S3+S4 continue?
(harder than it looks)
No, cannot be sure what state S4 had before failure.
Might have formed a new view with S1+S5, and committed some ops.</p> <p>When can Harp form a new view?</p> <ol><li>No other view possible.</li> <li>Know view # of most recent view.</li> <li>Know all ops from most recent view.
#1 is true if you have n+1 nodes in new view.
#2 is true if you have n+1 nodes that did not lose view # since last view.
View # stored on disk, so they just have to know disk is OK.
One of them <em>must</em> have been in the previous view.
So just take the highest view number.
And #3?
Need a disk image, and a log, that together reflect all operations
through the end of the previous view.
Perhaps from different servers, e.g. log from promoted witness,
disk from backup that failed multiple views ago.</li></ol> <p>Does Harp have performance benefits?
In Fig 5-1, why is Harp <em>faster</em> than non-replicated server?
How much win would we expect by substituting RPC for disk operations?</p> <p>Why graph x=load y=response-time?
Why does this graph make sense?
Why not just graph total time to perform X operations?
One reason is that systems sometimes get more/less efficient w/ high load.
And we care a lot how they perform w/ overload.</p> <p>Why does response time go up with load?
Why first gradual...
Queuing and random bursts?
And some ops more expensive than others, cause temp delays.
Then almost straight up?
Probably has hard limits, like disk I/Os per second.
Queue length diverges once offered load &gt; capacity</p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">lastUpdate:</span> <span class="time">4/6/2021, 2:11:34 PM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/note/mit/fen-bu-shi-xi-tong/07.html" class="prev">
        7.Guest lecturer: Russ Cox (Google/Go)
      </a></span> <span class="next"><a href="/note/mit/fen-bu-shi-xi-tong/09.html">
        9. Distributed Computing: Sequential consistency
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/note/assets/js/app.d5006f3d.js" defer></script><script src="/note/assets/js/2.65e5b2f3.js" defer></script><script src="/note/assets/js/89.cc281bcf.js" defer></script>
  </body>
</html>
