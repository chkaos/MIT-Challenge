(window.webpackJsonp=window.webpackJsonp||[]).push([[89],{466:function(e,t,a){"use strict";a.r(t);var n=a(45),s=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"_8-case-studies-replicated-file-system-harp"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_8-case-studies-replicated-file-system-harp"}},[e._v("#")]),e._v(" 8.Case Studies: Replicated File System -- Harp")]),e._v(" "),a("p",[e._v("Replication in the Harp File System\nLiskov, Ghemawat, Gruber, Johnson, Shrira, Williams\nSOSP 1991")]),e._v(" "),a("p",[e._v("Why are we reading this paper?\nHarp was the first complete replication system that dealt w/ partition\nIt's a complete case study of a replicated service (file server)\nIt uses Raft-like replication techniques")]),e._v(" "),a("p",[e._v("How could a 1991 paper still be worth reading?\nHarp introduced techniques that are still widely used\nThere are few papers describing complete replicated systems")]),e._v(" "),a("p",[e._v("The paper is a mix of fundamentals and incidentals\nWe care a lot about replication\nWe may not care much about NFS specifically\nBut we care a lot about the challenges faced when integrating\na real application with a replication protocol.\nAnd we care about where optimization is possible.")]),e._v(" "),a("p",[e._v("I'm going to focus on parts of Harp that aren't already present in Raft.\nBut note that Harp pre-dates Raft by 20+ years.\nRaft is, to a great extent, a tutorial on ideas pioneered by Harp.\nThough they differ in many details.")]),e._v(" "),a("p",[e._v("What does Harp paper explain that Raft paper does not?\nAdapting a complex service to state machine abstraction\ne.g. possibility of applying an operation twice\nLots of optimizations\npipelining of requests to backup\nwitness\nprimary-only execution of read-only operations using leases\nEfficient re-integration of re-started server with large state\nPower failure, including simultaneous failure of all servers\nEfficient persistence on disk")]),e._v(" "),a("p",[e._v("Harp authors had not implemented recovery yet\nEarlier paper (1988) describes View-stamped Replication:\nhttp://www.pmg.csail.mit.edu/papers/vr.pdf\nLater (2006) paper has clearer description, though a bit different:\nhttp://pmg.csail.mit.edu/papers/vr-revisited.pdf")]),e._v(" "),a("p",[e._v("Basic setup is familiar\nClients, primary, backup(s), witness(es).\nClient -> Primary\nPrimary -> Backups\nBackups -> Primary\nPrimary waits for all backups / promoted witnesses in current view\nCommit point\nPrimary -> execute and reply to Client\nPrimary -> tell Backups to Commit")]),e._v(" "),a("p",[e._v("Why are 2b+1 servers necessary to tolerate b failures?\nRequiring majority for each operation helps ensure no other partition.\nAnd majority intersection helps ensure state persists to future views.\nNot waiting for b allows tolerance of b failures.")]),e._v(" "),a("p",[e._v("What are Harp's witnesses?\nThe witnesses are one significant difference from Raft.\nThe b witnesses do not ordinarily hear about operations or keep state.\nWhy is that OK?\nb+1 of 2b+1 do have state\nSo any b failures leaves at least one live copy of state.\nWhy are the b witnesses needed at all?\nIf b replicas with state do fail, witnesses give the required b+1 majority.\nTo ensure that only one partition operates -- no split brain.\nSo, for a 3-server system, the witness is there to break ties about\nwhich partition is allowed to operate when primary and backup are\nin different partitions. The partition with the witness wins.")]),e._v(" "),a("p",[e._v('Does primary need to send operations to witnesses?\nThe primary must collect ACKs from a majority of the 2b+1 for every r/w operation.\nTo ensure that it is still the primary -- still in the majority partition.\nTo ensure that operation is on enough servers to intersect with any\nfuture majority that forms a new view.\nIf all backups are up, primary+backups are enough for that majority.\nIf m backups are down:\nPrimary must talk to m "promoted" witnesses to get its majority for each op.\nThose witnesses must record the op, to ensure overlap with any\nfuture majority.\nThus each "promoted" witness keeps a log.')]),e._v(" "),a("p",[e._v("So in a 2b+1 system, a view always has b+1 servers that the primary\nmust contact for each op, and that store each op.")]),e._v(" "),a("p",[e._v("Note: somewhat different from Raft\nRaft keeps sending each op to all servers, proceeds when majority answer\nSo leader must keep full log until failed server re-joins\nHarp eliminates failed server from view, doesn't send to it\nOnly witness has to keep a big log; has special plan (ram, disk, tape).\nThe bigger issue is that it can take a lot of work to bring\na re-joining replica up to date; careful design is required.")]),e._v(" "),a("p",[e._v("What's the story about the UPS?\nThis is one of the most interesting aspects of Harp's design\nEach server's power cord is  plugged into a UPS\nUPS has enough battery to run server for a few minutes\nUPS tells server (via serial port) when main A/C power fails\nServer writes dirty FS blocks and Harp log to disk, then shuts down")]),e._v(" "),a("p",[e._v("What does the UPS buy for Harp?\nEfficient protection against A/C power failure of ALL servers\nFor failures of up to b servers, replication is enough\nIf "),a("em",[e._v("all")]),e._v(" servers failed and lost state, that's more than b failures,\nso Harp has no guarantee (and indeed no state!)\nWith UPS:\nEach server can reply "),a("em",[e._v("without")]),e._v(" writing disk!\nBut still guarantees to retain latest state despite simultaneous power fail\nBut note:\nUPS does not protect against other causes of simultaneous failure\ne.g. bugs, earthquake\nHarp treats servers that re-start after UPS-protected crash differently\nthan those that re-start with crash that lost in-memory state\nBecause the latter may have forgotten "),a("em",[e._v("committed")]),e._v(" operations")]),e._v(" "),a("p",[e._v("Larger point, faced by every fault-tolerant system\nReplicas "),a("em",[e._v("must")]),e._v(" keep persistent state to deal w/ failure of all servers\nCommitted operations\nLatest view number, proposal number, &c\nMust persist this state before replying\nWriting every commit to disk is very slow!\n10 ms per disk write, so only 100 ops/second\nSo there are a few common patterns:\n1. Low throughput\n2. Batching, high delay\n3. Lossy or inconsistent recovery from simultaneous failure\n4. Batteries, flash, SSD w/ capacitor, &c")]),e._v(" "),a("p",[e._v("Let's talk about Harp's log management and operation execution\nPrimary and backup must apply client operations to their state\nState here is a file system -- directories, file, owners, permissions, &c\nHarp must mimic an ordinary NFS server to the client\ni.e. not forget about ops for which it has sent a reply")]),e._v(" "),a("p",[e._v("What is in a typical log record?\nClient's NFS operation (write, mkdir, chmod, &c)\nShadow state: modified i-nodes and directory content "),a("em",[e._v("after")]),e._v(" execution\nClient RPC request ID, for duplicate detection\nReply to send to client, for duplicate detection")]),e._v(" "),a("p",[e._v("Why does Harp have so many log pointers?\nFP most recent client request\nCP commit point (real in primary, latest heard in backup)\nAP highest update sent to disk\nLB disk has finished writing up to here\nGLB all nodes have completed disk up to here")]),e._v(" "),a("p",[e._v("Why the FP-CP gap?\nSo primary doesn't need to wait for ACKs from each backup\nbefore sending next operation to backups\nPrimary pipelines ops CP..FP to the backups.\nHigher throughput if concurrent client requests.")]),e._v(" "),a("p",[e._v("Why the AP-LB gap?\nAllows Harp to issue many ops as disk writes before waiting for disk\nThe disk is more efficient if it has lots of writes (e.g. arm scheduling)")]),e._v(" "),a("p",[e._v("What is the LB?\nThis replica has everything <= LB on disk.\nSo it won't need those log records again.")]),e._v(" "),a("p",[e._v("Why the LB-GLB gap?\nGLB is min(all servers' LBs).\nGLB is earliest record that "),a("em",[e._v("some")]),e._v(" server might need if it loses memory.")]),e._v(" "),a("p",[e._v("When does Harp execute a client operation?\nThere are two answers!")]),e._v(" "),a("ol",[a("li",[e._v("When operation arrives, primary figures out exactly what should happen.\nProduces resulting on-disk bytes for modified i-nodes, directories, &c.\nThis is the shadow state.\nThis happens before the CP, so the primary must consult recent\noperations in the log to find the latest file system state.")]),e._v(" "),a("li",[e._v("After the operation commits, primary and backup can apply it to\ntheir file systems.\nThey copy the log entry's shadow state to the file system;\nthey do not really execute the operation.\nAnd now the primary can reply to the client's RPC.")])]),e._v(" "),a("p",[e._v("Why does Harp split execution in this way?\nIf a server crashes and reboots, it is brought up to date by replaying\nlog entries it might have missed. Harp can't know exactly what the\nlast pre-crash operation was, so Harp may repeat some. It's not\ncorrect to fully execute some operations twice, e.g. file append.\nSo Harp log entries contain the "),a("em",[e._v("resulting")]),e._v(" state, which is what's applied.")]),e._v(" "),a("p",[e._v("The point: multiple replay means replication isn't transparent to the service.\nService must be modified to generate and accept the state\nmodifications that result from client operations.\nIn general, when applying replication to existing services,\nthe service must be modified to cope with multiple replay.")]),e._v(" "),a("p",[e._v("Can Harp primary execute read-only operations w/o replicating to backups?\ne.g. reading a file.\nWould be faster -- after all, there's no new data to replicate.\nWhat's the danger?\nHarp's idea: leases\nBackups promise not to form a new view for some time.\nPrimary can execute read-only ops locally for that time minus slop.\nDepends on reasonably synchronized clocks:\nPrimary and backup must have bounded disagreement\non how fast time passes.")]),e._v(" "),a("p",[e._v("What state should primary use when executing a read-only operation?\nDoes it have to wait for all previously arrived operations to commit?\nNo! That would be almost as slow as committing the read-only op.\nShould it look at state as of operation at FP, i.e. latest r/w operation?\nNo! That operation has not committed; not allowed to reveal its effects.\nThus Harp executes read-only ops with state as of CP.\nWhat if client sends a WRITE and (before WRITE finishes) a READ of same data?\nREAD may see data "),a("em",[e._v("before")]),e._v(" the WRITE!\nWhy is that OK?")]),e._v(" "),a("p",[e._v("How does failure recovery work?\nI.e. how does Harp recover replicated state during view change?")]),e._v(" "),a("p",[e._v("Setup for the following scenarios\n5 servers: 1 is usually primary, 2+3 are backups, 4+5 witnesses")]),e._v(" "),a("p",[e._v("Scenario:\nS1+S2+S3; then S1 crashes\nS2 is primary in new view (and S4 is promoted)\nWill S2 have every committed operation?\nWill S2 have every operation S1 received?\nWill S2's log tail be the same as S3's log tail?\nHow far back can S2 and S3 log tail differ?\nHow to cause S2 and S3's log to be the same?\nMust commit ops that appeared in both S2+S3 logs\nWhat about ops that appear in only one log?\nIn this scenario, can discard since could not have committed\nBut in general committed op might be visible in just one log\nFrom what point does promoted witness have to start keeping a log?")]),e._v(" "),a("p",[e._v("What if S1 crashed just before replying to a client?\nWill the client ever get a reply?")]),e._v(" "),a("p",[e._v("After S1 recovers, with intact disk, but lost memory.\nIt will be primary, but Harp can't immediately use its state or log.\nUnlike Raft, where leader only elected if it has the best log.\nHarp must replay log from promoted witness (S4)\nCould S1 have executed an op just before crashing\nthat the replicas didn't execute after taking over?\nNo, execution up to CP only, and CP is safe on S2+S3.")]),e._v(" "),a("p",[e._v("New scenario: S2 and S3 are partitioned (but still alive)\nCan S1+S4+S5 continue to process operations?\nYes, promoted witnesses S4+S5\nS4 moves to S2/S3 partition\nCan S1+S5 continue?\nNo, primary S1 doesn't get enough backup ACKs\nCan S2+S3+S4 continue?\nYes, new view copies log entries S4->S2, S4->S3, now S2 is primary\nNote:\nNew primary was missing many committed operations\nIn general some "),a("em",[e._v("committed")]),e._v(" operations may be on only one server")]),e._v(" "),a("p",[e._v("New scenario: S2 and S3 are partitioned (but still alive)\nS4 crashes, loses memory contents, reboots in S2/S3 partition\nCan they continue?\nOnly if there wasn't another view that formed and committed more ops\nHow to detect?\nDepends on what S4's on-disk view # says.\nOK if S4's disk view # is same as S2+S3's.\nNo new views formed.\nS2+S3 must have heard about all committed ops in old view.")]),e._v(" "),a("p",[e._v("Everybody suffers a power failure.\nS4 disk and memory are lost, but it does re-start after repair.\nS1 and S5 never recover.\nS2 and S3 save everything on disk, re-start just fine.\nCan S2+S3+S4 continue?\n(harder than it looks)\nNo, cannot be sure what state S4 had before failure.\nMight have formed a new view with S1+S5, and committed some ops.")]),e._v(" "),a("p",[e._v("When can Harp form a new view?")]),e._v(" "),a("ol",[a("li",[e._v("No other view possible.")]),e._v(" "),a("li",[e._v("Know view # of most recent view.")]),e._v(" "),a("li",[e._v("Know all ops from most recent view.\n#1 is true if you have n+1 nodes in new view.\n#2 is true if you have n+1 nodes that did not lose view # since last view.\nView # stored on disk, so they just have to know disk is OK.\nOne of them "),a("em",[e._v("must")]),e._v(" have been in the previous view.\nSo just take the highest view number.\nAnd #3?\nNeed a disk image, and a log, that together reflect all operations\nthrough the end of the previous view.\nPerhaps from different servers, e.g. log from promoted witness,\ndisk from backup that failed multiple views ago.")])]),e._v(" "),a("p",[e._v("Does Harp have performance benefits?\nIn Fig 5-1, why is Harp "),a("em",[e._v("faster")]),e._v(" than non-replicated server?\nHow much win would we expect by substituting RPC for disk operations?")]),e._v(" "),a("p",[e._v("Why graph x=load y=response-time?\nWhy does this graph make sense?\nWhy not just graph total time to perform X operations?\nOne reason is that systems sometimes get more/less efficient w/ high load.\nAnd we care a lot how they perform w/ overload.")]),e._v(" "),a("p",[e._v("Why does response time go up with load?\nWhy first gradual...\nQueuing and random bursts?\nAnd some ops more expensive than others, cause temp delays.\nThen almost straight up?\nProbably has hard limits, like disk I/Os per second.\nQueue length diverges once offered load > capacity")])])}),[],!1,null,null,null);t.default=s.exports}}]);