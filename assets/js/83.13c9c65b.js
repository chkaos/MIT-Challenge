(window.webpackJsonp=window.webpackJsonp||[]).push([[83],{460:function(e,n,t){"use strict";t.r(n);var a=t(45),r=Object(a.a)({},(function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"_2-infrastructure-rpc-and-threads"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-infrastructure-rpc-and-threads"}},[e._v("#")]),e._v(" 2: Infrastructure: RPC and threads")]),e._v(" "),t("p",[e._v("Remote Procedure Call (RPC)\na key piece of distrib sys machinery; all the labs use RPC\ngoal: easy-to-program network communication\nhides most details of client/server communication\nclient call is much like ordinary procedure call\nserver handlers are much like ordinary procedures\nRPC is widely used!")]),e._v(" "),t("p",[e._v("RPC ideally makes net communication look just like fn call:\nClient:\nz = fn(x, y)\nServer:\nfn(x, y) {\ncompute\nreturn z\n}\nRPC aims for this level of transparency")]),e._v(" "),t("p",[e._v("Examples from lab 1:\nDoJob\nRegister")]),e._v(" "),t("p",[e._v("RPC message diagram:\nClient             Server\nrequest---\x3e\n<---response")]),e._v(" "),t("p",[e._v("Software structure\nclient app         handlers\nstubs           dispatcher\nRPC lib           RPC lib\nnet  ------------ net")]),e._v(" "),t("p",[e._v("A few details:\nWhich server function (handler) to call?\nMarshalling: format data into packets\nTricky for arrays, pointers, objects, &c\nGo's RPC library is pretty powerful!\nsome things you cannot pass: e.g., channels, functions\nBinding: how does client know who to talk to?\nMaybe client supplies server host name\nMaybe a name service maps service names to best server host\nThreads:\nClient often has many threads, so > 1 call outstanding, match up replies\nHandlers may be slow, so server often runs each in a thread")]),e._v(" "),t("p",[e._v("RPC problem: what to do about failures?\ne.g. lost packet, broken network, slow server, crashed server")]),e._v(" "),t("p",[e._v("What does a failure look like to the client RPC library?\nClient never sees a response from the server\nClient does "),t("em",[e._v("not")]),e._v(" know if the server saw the request!\nMaybe server/net failed just before sending reply\n[diagram of lost reply]")]),e._v(" "),t("p",[e._v('Simplest scheme: "at least once" behavior\nRPC library waits for response for a while\nIf none arrives, re-send the request\nDo this a few times\nStill no response -- return an error to the application')]),e._v(" "),t("p",[e._v('Q: is "at least once" easy for applications to cope with?')]),e._v(" "),t("p",[e._v('Simple problem w/ at least once:\nclient sends "deduct $10 from bank account"')]),e._v(" "),t("p",[e._v('Q: what can go wrong with this client program?\nPut("k", 10) -- an RPC to set key\'s value in a DB server\nPut("k", 20) -- client then does a 2nd Put to same key\n[diagram, timeout, re-send, original arrives very late]')]),e._v(" "),t("p",[e._v("Q: is at-least-once ever OK?\nyes: if it's OK to repeat operations, e.g. read-only op\nyes: if application has its own plan for coping w/ duplicates\nwhich you will need for Lab 1")]),e._v(" "),t("p",[e._v('Better RPC behavior: "at most once"\nidea: server RPC code detects duplicate requests\nreturns previous reply instead of re-running handler\nQ: how to detect a duplicate request?\nclient includes unique ID (XID) with each request\nuses same XID for re-send\nserver:\nif seen[xid]:\nr = old[xid]\nelse\nr = handler()\nold[xid] = r\nseen[xid] = true')]),e._v(" "),t("p",[e._v('some at-most-once complexities\nthis will come up in labs 2 and on\nhow to ensure XID is unique?\nbig random number?\ncombine unique client ID (ip address?) with sequence #?\nserver must eventually discard info about old RPCs\nwhen is discard safe?\nidea:\nunique client IDs\nper-client RPC sequence numbers\nclient includes "seen all replies <= X" with every RPC\nmuch like TCP sequence #s and acks\nor only allow client one outstanding RPC at a time\narrival of seq+1 allows server to discard all <= seq\nor client agrees to keep retrying for < 5 minutes\nserver discards after 5+ minutes\nhow to handle dup req while original is still executing?\nserver doesn\'t know reply yet; don\'t want to run twice\nidea: "pending" flag per executing RPC; wait or ignore')]),e._v(" "),t("p",[e._v("What if an at-most-once server crashes and re-starts?\nif at-most-once duplicate info in memory, server will forget\nand accept duplicate requests after re-start\nmaybe it should write the duplicate info to disk?\nmaybe replica server should also replicate duplicate info?")]),e._v(" "),t("p",[e._v('What about "exactly once"?\nat-most-once plus unbounded retries plus fault-tolerant service\nLab 3')]),e._v(" "),t("p",[e._v("Go RPC is \"at-most-once\"\nopen TCP connection\nwrite request to TCP connection\nTCP may retransmit, but server's TCP will filter out duplicates\nno retry in Go code (i.e. will NOT create 2nd TCP connection)\nGo RPC code returns an error if it doesn't get a reply\nperhaps after a timeout (from TCP)\nperhaps server didn't see request\nperhaps server processed request but server/net failed before reply came back")]),e._v(" "),t("p",[e._v("Go RPC's at-most-once isn't enough for Lab 1\nit only applies to a single RPC call\nif worker doesn't respond, the master re-send to it to another worker\nbut original worker may have not failed, and is working on it too\nGo RPC can't detect this kind of duplicate\nNo problem in lab 1, which handles at application level\nLab 2 will explicitly detect duplicates")]),e._v(" "),t("p",[e._v("Threads\nthreads are a fundamental server structuring tool\nyou'll use them a lot in the labs\nthey can be tricky\nuseful with RPC\nGo calls them goroutines; everyone else calls them threads")]),e._v(" "),t("p",[e._v('Thread = "thread of control"\nthreads allow one program to (logically) do many things at once\nthe threads share memory\neach thread includes some per-thread state:\nprogram counter, registers, stack')]),e._v(" "),t("p",[e._v("Threading challenges:\nsharing data\ntwo threads modify the same variable at same time?\none thread reads data that another thread is changing?\nthese problems are often called races\nneed to protect invariants on shared data\nuse Go sync.Mutex\ncoordination between threads\ne.g. wait for all Map threads to finish\nuse Go channels\ndeadlock\nthread 1 is waiting for thread 2\nthread 2 is waiting for thread 1\neasy detectable (unlike races)\nlock granularity\ncoarse-grained -> simple, but little concurrency/parallelism\nfine-grained -> more concurrency, more races and deadlocks\nlet's look at a toy RPC package to illustrate these problems")]),e._v(" "),t("p",[e._v("look at today's handout -- l-rpc.go\nit's a simplified RPC system\nillustrates threads, mutexes, channels\nit's a toy, though it does run\nassumes connection already open\nonly supports an integer arg, integer reply\nomits error checks")]),e._v(" "),t("p",[e._v("struct ToyClient\nclient RPC state\nmutex per ToyClient\nconnection to server (e.g. TCP socket)\nxid -- unique ID per call, to match reply to caller\npending[] -- chan per thread waiting in Call()\nso client knows what to do with each arriving reply")]),e._v(" "),t("p",[e._v('Call\napplication calls reply := client.Call(procNum, arg)\nprocNum indicates what function to run on server\nWriteRequest knows the format of an RPC msg\nbasically just the arguments turned into bits in a packet\nQ: why the mutex in Call()? what does mu.Lock() do?\nQ: could we move "xid := tc.xid" outside the critical section?\nafter all, we are not changing anything\n[diagram to illustrate]\nQ: do we need to WriteRequest inside the critical section?\nnote: Go says you are responsible for preventing concurrent map ops\nthat\'s one reason the update to pending is locked')]),e._v(" "),t("p",[e._v("Listener\nruns as a background thread\nwhat is <- doing?\nnot quite right that it may need to wait on chan for caller")]),e._v(" "),t("p",[e._v("Back to Call()...")]),e._v(" "),t("p",[e._v("Q: what if reply comes back very quickly?\ncould Listener() see reply before pending[xid] entry exists?\nor before caller is waiting for channel?")]),e._v(" "),t("p",[e._v("Q: should we put reply:=<-done inside the critical section?\nwhy is it OK outside? after all, two threads use it.")]),e._v(" "),t("p",[e._v("Q: why mutex per ToyClient, rather than single mutex per whole RPC pkg?")]),e._v(" "),t("p",[e._v("Server's Dispatcher()\nnote that the Dispatcher echos the xid back to the client\nso that Listener knows which Call to wake up\nQ: why run the handler in a separate thread?\nQ: is it a problem that the dispatcher can reply out of order?")]),e._v(" "),t("p",[e._v("main()\nnote registering handler in handlers[]\nwhat will the program print?")]),e._v(" "),t("p",[e._v("Q: when to use channels vs shared memory + locks?\nhere is my opinion\nuse channels when you want one thread to explicitly wait for another\noften wait for a result, or wait for the next request\ne.g. when client Call() waits for Listener()\nuse shared memory and locks when the threads are not intentionally\ndirectly interacting, but just happen to r/w the same data\ne.g. when Call() uses tc.xid\nbut: they are fundamentally equivalent; either can always be used.")]),e._v(" "),t("p",[e._v("Go's \"memory model\" requires explicit synchronization to communicate!\nThis code is not correct:\nvar x int\ndone := false\ngo func() { x = f(...); done = true }\nwhile done == false { }\nit's very tempting to write, but the Go spec says it's undefined\nuse a channel or sync.WaitGroup instead")]),e._v(" "),t("p",[e._v("Study the Go tutorials on goroutines and channels")])])}),[],!1,null,null,null);n.default=r.exports}}]);