(window.webpackJsonp=window.webpackJsonp||[]).push([[97],{474:function(e,t,s){"use strict";s.r(t);var n=s(45),a=Object(n.a)({},(function(){var e=this,t=e.$createElement,s=e._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[s("h1",{attrs:{id:"_16-scaling-memcache-at-facebook"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_16-scaling-memcache-at-facebook"}},[e._v("#")]),e._v(" 16.Scaling Memcache at Facebook")]),e._v(" "),s("p",[e._v("Scaling Memcache at Facebook, by Nishtala et al, NSDI 2013")]),e._v(" "),s("p",[e._v("why are we reading this paper?\nit's an experience paper, not about new ideas/techniques\nthree ways to read it:\ncautionary tale of problems from not taking consistency seriously\nimpressive story of super high capacity from mostly-off-the-shelf s/w\nfundamental struggle between performance and consistency\nwe can argue with their design, but not their success")]),e._v(" "),s("p",[e._v("how do web sites scale up with growing load?\na typical story of evolution over time:")]),e._v(" "),s("ol",[s("li",[e._v("one machine, web server, application, DB\nDB stores on disk, crash recovery, transactions, SQL\napplication queries DB, formats, HTML, &c\nbut the load grows, your PHP application takes too much CPU time")]),e._v(" "),s("li",[e._v("many web FEs, one shared DB\nan easy change, since web server + app already separate from storage\nFEs are stateless, all sharing (and concurrency control) via DB\nbut the load grows; add more FEs; soon single DB server is bottleneck")]),e._v(" "),s("li",[e._v("many web FEs, data sharded over cluster of DBs\npartition data by key over the DBs\napp looks at key (e.g. user), chooses the right DB\ngood DB parallelism if no data is super-popular\npainful -- cross-shard transactions and queries probably don't work\nhard to partition too finely\nbut DBs are slow, even for reads, why not cache read requests?")]),e._v(" "),s("li",[e._v("many web FEs, many caches for reads, many DBs for writes\ncost-effective b/c read-heavy and memcached 10x faster than a DB\nmemcached just an in-memory hash table, very simple\ncomplex b/c DB and memcacheds can get out of sync\n(next bottleneck will be DB writes -- hard to solve)")])]),e._v(" "),s("p",[e._v('the big facebook infrastructure picture\nlots of users, friend lists, status, posts, likes, photos\nfresh/consistent data apparently not critical\nbecause humans are tolerant?\nhigh load: billions of operations per second\nthat\'s 10,000x the throughput of one DB server\nmultiple data centers (at least west and east coast)\neach data center -- "region":\n"real" data sharded over MySQL DBs\nmemcached layer (mc)\nweb servers (clients of memcached)\neach data center\'s DBs contain full replica\nwest coast is master, others are slaves via MySQL async log replication')]),e._v(" "),s("p",[e._v('how do FB apps use mc?\nread:\nv = get(k) (computes hash(k) to choose mc server)\nif v is nil {\nv = fetch from DB\nset(k, v)\n}\nwrite:\nv = new value\nsend k,v to DB\ndelete(k)\napplication determines relationship of mc to DB\nmc doesn\'t know anything about DB\nFB uses mc as a "look-aside" cache\nreal data is in the DB\ncached value (if any) should be same as DB')]),e._v(" "),s("p",[e._v("what does FB store in mc?\npaper does not say\nmaybe userID -> name; userID -> friend list; postID -> text; URL -> likes\nbasically copies of data from DB")]),e._v(" "),s("p",[e._v('paper lessons:\nlook-aside is much trickier than it looks -- consistency\npaper is trying to integrate mutually-oblivious storage layers\ncache is critical:\nnot really about reducing user-visible delay\nmostly about surviving huge load!\ncache misses and failures can create intolerable DB load\nthey can tolerate modest staleness: no freshness guarantee\nstale data nevertheless a big headache\nwant to avoid unbounded staleness (e.g. missing a delete() entirely)\nwant read-your-own-writes\neach performance fix brings a new source of staleness\nhuge "fan-out" => parallel fetch, in-cast congestion')]),e._v(" "),s("p",[e._v("let's talk about performance first\nmajority of paper is about avoiding stale data\nbut staleness only arose from performance design")]),e._v(" "),s("p",[e._v("performance comes from parallel get()s by many mc servers\ndriven by parallel processing of HTTP requests by many web servers\ntwo basic parallel strategies for storage: partition vs replication")]),e._v(" "),s("p",[e._v("will partition or replication yield most mc throughput?\npartition: server i, key k -> mc server hash(k)\nreplicate: server i, key k -> mc server hash(i)\npartition is more memory efficient (one copy of each k/v)\npartition works well if no key is very popular\npartition forces each web server to talk to many mc servers (overhead)\nreplication works better if a few keys are very popular")]),e._v(" "),s("p",[e._v("performance and regions (Section 5)")]),e._v(" "),s("p",[e._v("Q: what is the point of regions -- multiple complete replicas?\nlower RTT to users (east coast, west coast)\nparallel reads of popular data due to replication\n(note DB replicas help only read performance, no write performance)\nmaybe hot replica for main site failure?")]),e._v(" "),s("p",[e._v("Q: why not partition users over regions?\ni.e. why not east-coast users' data in east-coast region, &c\nsocial net -> not much locality\nvery different from e.g. e-mail")]),e._v(" "),s("p",[e._v("Q: why OK performance despite all writes forced to go to the master region?\nwrites would need to be sent to all regions anyway -- replicas\nusers probably wait for round-trip to update DB in master region\nonly 100ms, not so bad\nusers do not wait for all effects of writes to finish\ni.e. for all stale cached values to be deleted")]),e._v(" "),s("p",[e._v("performance within a region (Section 4)")]),e._v(" "),s("p",[e._v("multiple mc clusters "),s("em",[e._v("within")]),e._v(" each region\ncluster == complete set of mc cache servers\ni.e. a replica, at least of cached data")]),e._v(" "),s("p",[e._v("why multiple clusters per region?\nwhy not add more and more mc servers to a single cluster?")]),e._v(" "),s("ol",[s("li",[e._v("adding mc servers to cluster doesn't help single popular keys\nreplicating (one copy per cluster) does help")]),e._v(" "),s("li",[e._v("more mcs in cluster -> each client req talks to more servers\nand more in-cast congestion at requesting web servers\nclient requests fetch 20 to 500 keys! over many mc servers\nMUST request them in parallel (otherwise total latency too large)\nso all replies come back at the same time\nnetwork switches, NIC run out of buffers")]),e._v(" "),s("li",[e._v("hard to build network for single big cluster\nuniform client/server access\nso cross-section b/w must be large -- expensive\ntwo clusters -> 1/2 the cross-section b/w")])]),e._v(" "),s("p",[e._v('but -- replicating is a waste of RAM for less-popular items\n"regional pool" shared by all clusters\nunpopular objects (no need for many copies)\ndecided by '),s("em",[e._v("type")]),e._v(" of object\nfrees RAM to replicate more popular objects")]),e._v(" "),s("p",[e._v('bringing up new mc cluster was a serious performance problem\nnew cluster has 0% hit rate\nif clients use it, will generate big spike in DB load\nif ordinarily 1% miss rate, and (let\'s say) 2 clusters,\nadding "cold" third cluster will causes misses for 33% of ops.\ni.e. 30x spike in DB load!\nthus the clients of new cluster first get() from existing cluster (4.3)\nand set() into new cluster\nbasically lazy copy of existing cluster to new cluster\nbetter 2x load on existing cluster than 30x load on DB')]),e._v(" "),s("p",[e._v("important practical networking problems:\nn^2 TCP connections is too much state\nthus UDP for client get()s\nUDP is not reliable or ordered\nthus TCP for client set()s\nand mcrouter to reduce n in n^2\nsmall request per packet is not efficient (for TCP or UDP)\nper-packet overhead (interrupt &c) is too high\nthus mcrouter batches many requests into each packet")]),e._v(" "),s("p",[e._v("mc server failure?\ncan't have DB servers handle the misses -- too much load\ncan't shift load to one other mc server -- too much\ncan't re-partition all data -- time consuming\nGutter -- pool of idle servers, clients only use after mc server fails")]),e._v(" "),s("p",[e._v("The Question:\nwhy don't clients send invalidates to Gutter servers?\nmy guess: would double delete() traffic\nand send too many delete()s to small gutter pool\nsince any key might be in the gutter pool")]),e._v(" "),s("p",[e._v('thundering herd\none client updates DB and delete()s a key\nlots of clients get() but miss\nthey all fetch from DB\nthey all set()\nnot good: needless DB load\nmc gives just the first missing client a "lease"\nlease = permission to refresh from DB\nmc tells others "try get() again in a few milliseconds"\neffect: only one client reads the DB and does set()\nothers re-try get() later and hopefully hit')]),e._v(" "),s("p",[e._v("let's talk about consistency now")]),e._v(" "),s("p",[e._v("the big truth\nhard to get both consistency (== freshness) and performance\nperformance for reads = many copies\nmany copies = hard to keep them equal")]),e._v(" "),s("p",[e._v("what is their consistency goal?\n"),s("em",[e._v("not")]),e._v(' read sees latest write\nsince not guaranteed across clusters\nmore like "not more than a few seconds stale"\ni.e. eventual\n'),s("em",[e._v("and")]),e._v(" writers see their own writes\nread-your-own-writes is a big driving force")]),e._v(" "),s("p",[e._v("first, how are DB replicas kept consistent across regions?\none region is master\nmaster DBs distribute log of updates to DBs in slave regions\nslave DBs apply\nslave DBs are complete replicas (not caches)\nDB replication delay can be considerable (many seconds)")]),e._v(" "),s("p",[e._v("how do we feel about the consistency of the DB replication scheme?\ngood: eventual consistency, b/c single ordered write stream\nbad: longish replication delay -> stale reads")]),e._v(" "),s("p",[e._v("how do they keep mc content consistent w/ DB content?")]),e._v(" "),s("ol",[s("li",[e._v("DBs send invalidates (delete()s) to all mc servers that might cache")]),e._v(" "),s("li",[e._v("writing client also invalidates mc in local cluster\nfor read-your-writes")])]),e._v(" "),s("p",[e._v("why did they have consistency problems in mc?\nclient code to copy DB to mc wasn't atomic:\n1. writes: DB update ... mc delete()\n2. read miss: DB read ... mc set()\nso "),s("em",[e._v("concurrent")]),e._v(" clients had races")]),e._v(" "),s("p",[e._v("what were the races and fixes?")]),e._v(" "),s("p",[e._v("Race 1:\nk not in cache\nC1 get(k), misses\nC1 v = read k from DB\nC2 updates k in DB\nC2 and DB delete(k) -- does nothing\nC1 set(k, v)\nnow mc has stale data, delete(k) has already happened\nwill stay stale indefinitely, until key is next written\nsolved with leases -- C1 gets a lease, but C2's delete() invalidates lease,\nso mc ignores C1's set\nkey still missing, so next reader will refresh it from DB")]),e._v(" "),s("p",[e._v("Race 2:\nduring cold cluster warm-up\nremember clients try get() in warm cluster, copy to cold cluster\nk starts with value v1\nC1 updates k to v2 in DB\nC1 delete(k) -- in cold cluster\nC2 get(k), miss -- in cold cluster\nC2 v1 = get(k) from warm cluster, hits\nC2 set(k, v1) into cold cluster\nnow mc has stale v1, but delete() has already happened\nwill stay stale indefinitely, until key is next written\nsolved with two-second hold-off, just used on cold clusters\nafter C1 delete(), cold ignores set()s for two seconds\nby then, delete() will propagate via DB to warm cluster")]),e._v(" "),s("p",[e._v('Race 3:\nk starts with value v1\nC1 is in a slave region\nC1 updates k=v2 in master DB\nC1 delete(k) -- local region\nC1 get(k), miss\nC1 read local DB  -- sees v1, not v2!\nlater, v2 arrives from master DB\nsolved by "remote mark"\nC1 delete() marks key "remote"\nget()/miss yields "remote"\ntells C1 to read from '),s("em",[e._v("master")]),e._v(' region\n"remote" cleared when new data arrives from master region')]),e._v(" "),s("p",[e._v("Q: aren't all these problems caused by clients copying DB data to mc?\nwhy not instead have DB send new values to mc, so clients only read mc?\nthen there would be no racing client updates &c, just ordered writes\nA:")]),e._v(" "),s("ol",[s("li",[e._v("DB doesn't generally know how to compute values for mc\ngenerally client app code computes them from DB results,\ni.e. mc content is often not simply a literal DB record")]),e._v(" "),s("li",[e._v("would increase read-your-own writes delay")]),e._v(" "),s("li",[e._v("DB doesn't know what's cached, would end up sending lots\nof values for keys that aren't cached")])]),e._v(" "),s("p",[e._v("PNUTS does take this alternate approach of master-updates-all-copies")]),e._v(" "),s("p",[e._v("FB/mc lessons for storage system designers?\ncache is vital to throughput survival, not just a latency tweak\nneed flexible tools for controlling partition vs replication\nneed better ideas for integrating storage layers with consistency")])])}),[],!1,null,null,null);t.default=a.exports}}]);