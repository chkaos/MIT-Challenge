(window.webpackJsonp=window.webpackJsonp||[]).push([[85],{462:function(e,t,a){"use strict";a.r(t);var r=a(45),s=Object(r.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"_4-fault-tolerance-fds-case-study"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-fault-tolerance-fds-case-study"}},[e._v("#")]),e._v(" 4.Fault Tolerance: FDS Case Study")]),e._v(" "),a("p",[e._v("Flat Datacenter Storage\nNightingale, Elson, Fan, Hofmann, Howell, Suzue\nOSDI 2012")]),e._v(" "),a("p",[e._v("why are we looking at this paper?\nLab 2 wants to be like this when it grows up\nthough details are all different\nfantastic performance -- world record cluster sort\ngood systems paper -- details from apps all the way to network")]),e._v(" "),a("p",[e._v("what is FDS?\na cluster storage system\nstores giant blobs -- 128-bit ID, multi-megabyte content\nclients and servers connected by network with high bisection bandwidth\nfor big-data processing (like MapReduce)\ncluster of 1000s of computers processing data in parallel")]),e._v(" "),a("p",[e._v('high-level design -- a common pattern\nlots of clients\nlots of storage servers ("tractservers")\npartition the data\nmaster ("metadata server") controls partitioning\nreplica groups for reliability')]),e._v(" "),a("p",[e._v("why is this high-level design useful?\n1000s of disks of space\nstore giant blobs, or many big blobs\n1000s of servers/disks/arms of parallel throughput\ncan expand over time -- reconfiguration\nlarge pool of storage servers for instant replacement after failure")]),e._v(" "),a("p",[e._v("motivating app: MapReduce-style sort\na mapper reads its split 1/Mth of the input file (e.g., a tract)\nmap emits a <key, record> for each record in split\nmap partitions keys among R intermediate files  (M*R intermediate files in total)\na reducer reads 1 of R intermediate files produced by each mapper\nreads M intermediate files (of 1/R size)\nsorts its input\nproduces 1/Rth of the final sorted output file  (R blobs)\nFDS sort\nFDS sort does not store the intermediate files in FDS\na client is both a mapper and reducer\nFDS sort is not locality-aware\nin mapreduce, master schedules workers on machine that are close to the data\ne.g.,  in same cluster\nlater versions of FDS sort uses more fine-grained work assignment\ne.g., mapper doesn't get 1/N of the input file but something smaller\ndeals better with stragglers")]),e._v(" "),a("p",[e._v("The Abstract's main claims are about performance.\nThey set the world-record for disk-to-disk sorting in 2012 for MinuteSort\n1,033 disks and 256 computers (136 tract servers, 120 clients)\n1,401 Gbyte in 59.4s")]),e._v(" "),a("p",[e._v("Q: does the abstract's 2 GByte/sec per client seem impressive?\nhow fast can you read a file from Athena AFS? (abt 10 MB/sec)\nhow fast can you read a typical hard drive?\nhow fast can typical networks move data?")]),e._v(" "),a("p",[e._v("Q: abstract claims recover from lost disk (92 GB) in 6.2 seconds\nthat's 15 GByte / sec\nimpressive?\nhow is that even possible? that's 30x the speed of a disk!\nwho might care about this metric?")]),e._v(" "),a("p",[e._v("what should we want to know from the paper?\nAPI?\nlayout?\nfinding data?\nadd a server?\nreplication?\nfailure handling?\nfailure model?\nconsistent reads/writes? (i.e. does a read see latest write?)\nconfig mgr failure handling?\ngood performance?\nuseful for apps?")]),e._v(" "),a("ul",[a("li",[e._v("API\nFigure 1\n128-bit blob IDs\nblobs have a length\nonly whole-tract read and write -- 8 MB")])]),e._v(" "),a("p",[e._v("Q: why are 128-bit blob IDs a nice interface?\nwhy not file names?")]),e._v(" "),a("p",[e._v("Q: why do 8 MB tracts make sense?\n(Figure 3...)")]),e._v(" "),a("p",[e._v("Q: what kinds of client applications is the API aimed at?\nand not aimed at?")]),e._v(" "),a("ul",[a("li",[e._v("Layout: how do they spread data over the servers?\nSection 2.2\nbreak each blob into 8 MB tracts\nTLT maintained by metadata server\nhas n entries\nfor blob b and tract t, i = (hash(b) + t) mod n\nTLT[i] contains list of tractservers w/ copy of the tract\nclients and servers all have copies of the latest TLT table")])]),e._v(" "),a("p",[e._v('Example four-entry TLT with no replication:\n0: S1\n1: S2\n2: S3\n3: S4\nsuppose hash(27) = 2\nthen the tracts of blob 27 are laid out:\nS1: 2 6\nS2: 3 7\nS3: 0 4 8\nS4: 1 5 ...\nFDS is "striping" blobs over servers at tract granularity')]),e._v(" "),a("p",[e._v("Q: why have tracts at all? why not store each blob on just one server?\nwhat kinds of apps will benefit from striping?\nwhat kinds of apps won't?")]),e._v(" "),a("p",[e._v("Q: how fast will a client be able to read a single tract?")]),e._v(" "),a("p",[e._v("Q: where does the abstract's single-client 2 GB number come from?")]),e._v(" "),a("p",[e._v("Q: why not the UNIX i-node approach?\nstore an array per blob, indexed by tract #, yielding tractserver\nso you could make per-tract placement decisions\ne.g. write new tract to most lightly loaded server")]),e._v(" "),a("p",[e._v("Q: why not hash(b + t)?")]),e._v(" "),a("p",[e._v("Q: how many TLT entries should there be?\nhow about n = number of tractservers?\nwhy do they claim this works badly? Section 2.2")]),e._v(" "),a("p",[e._v("The system needs to choose server pairs (or triplets &c) to put in TLT entries\nFor replication\nSection 3.3")]),e._v(" "),a("p",[e._v("Q: how about\n0: S1 S2\n1: S2 S1\n2: S3 S4\n3: S4 S3\n...\nWhy is this a bad idea?\nHow long will repair take?\nWhat are the risks if two servers fail?")]),e._v(" "),a("p",[e._v("Q: why is the paper's n^2 scheme better?\nTLT with n^2 entries, with every server pair occuring once\n0: S1 S2\n1: S1 S3\n2: S1 S4\n3: S2 S1\n4: S2 S3\n5: S2 S4\n...\nHow long will repair take?\nWhat are the risks if two servers fail?")]),e._v(" "),a("p",[e._v("Q: why do they actually use a minimum replication level of 3?\nsame n^2 table as before, third server is randomly chosen\nWhat effect on repair time?\nWhat effect on two servers failing?\nWhat if three disks fail?")]),e._v(" "),a("ul",[a("li",[a("p",[e._v("Adding a tractserver\nTo increase the amount of disk space / parallel throughput\nMetadata server picks some random TLT entries\nSubstitutes new server for an existing server in those TLT entries")])]),e._v(" "),a("li",[a("p",[e._v("How do they maintain n^2 plus one arrangement as servers leave join?\nUnclear.")])])]),e._v(" "),a("p",[e._v("Q: how long will adding a tractserver take?")]),e._v(" "),a("p",[e._v("Q: what about client writes while tracts are being transferred?\nreceiving tractserver may have copies from client(s) and from old srvr\nhow does it know which is newest?")]),e._v(" "),a("p",[e._v("Q: what if a client reads/writes but has an old tract table?")]),e._v(" "),a("ul",[a("li",[e._v("Replication\nA writing client sends a copy to each tractserver in the TLT.\nA reading client asks one tractserver.")])]),e._v(" "),a("p",[e._v("Q: why don't they send writes through a primary?")]),e._v(" "),a("p",[e._v("Q: what problems are they likely to have because of lack of primary?\nwhy weren't these problems show-stoppers?")]),e._v(" "),a("ul",[a("li",[e._v("What happens after a tractserver fails?\nMetadata server stops getting heartbeat RPCs\nPicks random replacement for each TLT entry failed server was in\nNew TLT gets a new version number\nReplacement servers fetch copies")])]),e._v(" "),a("p",[e._v("Example of the tracts each server holds:\nS1: 0 4 8 ...\nS2: 0 1 ...\nS3: 4 3 ...\nS4: 8 2 ...")]),e._v(" "),a("p",[e._v("Q: why not just pick one replacement server?")]),e._v(" "),a("p",[e._v("Q: how long will it take to copy all the tracts?")]),e._v(" "),a("p",[e._v("Q: if a tractserver's net breaks and is then repaired, might srvr serve old data?")]),e._v(" "),a("p",[e._v("Q: if a server crashes and reboots with disk intact, can contents be used?\ne.g. if it only missed a few writes?\n3.2.1's \"partial failure recovery\"\nbut won't it have already been replaced?\nhow to know what writes it missed?")]),e._v(" "),a("p",[e._v("Q: when is it better to use 3.2.1's partial failure recovery?")]),e._v(" "),a("ul",[a("li",[e._v("What happens when the metadata server crashes?")])]),e._v(" "),a("p",[e._v("Q: while metadata server is down, can the system proceed?")]),e._v(" "),a("p",[e._v("Q: is there a backup metadata server?")]),e._v(" "),a("p",[e._v("Q: how does rebooted metadata server get a copy of the TLT?")]),e._v(" "),a("p",[e._v("Q: does their scheme seem correct?\nhow does the metadata server know it has heard from all tractservers?\nhow does it know all tractservers were up to date?")]),e._v(" "),a("ul",[a("li",[e._v("Random issues")])]),e._v(" "),a("p",[e._v("Q: is the metadata server likely to be a bottleneck?")]),e._v(" "),a("p",[e._v("Q: why do they need the scrubber application mentioned in 2.3?\nwhy don't they delete the tracts when the blob is deleted?\ncan a blob be written after it is deleted?")]),e._v(" "),a("ul",[a("li",[e._v("Performance")])]),e._v(" "),a("p",[e._v("Q: how do we know we're seeing \"good\" performance?\nwhat's the best you can expect?")]),e._v(" "),a("p",[e._v("Q: limiting resource for 2 GB / second single-client?")]),e._v(" "),a("p",[e._v("Q: Figure 4a: why starts low? why goes up? why levels off?\nwhy does it level off at that particular performance?")]),e._v(" "),a("p",[e._v("Q: Figure 4b shows random r/w as fast as sequential (Figure 4a).\nis this what you'd expect?")]),e._v(" "),a("p",[e._v("Q: why are writes slower than reads with replication in Figure 4c?")]),e._v(" "),a("p",[e._v("Q: where does the 92 GB in 6.2 seconds come from?\nTable 1, 4th column\nthat's 15 GB / second, both read and written\n1000 disks, triple replicated, 128 servers?\nwhat's the limiting resource? disk? cpu? net?")]),e._v(" "),a("p",[e._v("How big is each sort bucket?\ni.e. is the sort of each bucket in-memory?\n1400 GB total\n128 compute servers\nbetween 12 and 96 GB of RAM each\nhmm, say 50 on average, so total RAM may be 6400 GB\nthus sort of each bucket is in memory, does not write passes to FDS\nthus total time is just four transfers of 1400 GB\nclient limit: 128 * 2 GB/s = 256 GB / sec\ndisk limit: 1000 * 50 MB/s = 50 GB / sec\nthus bottleneck is likely to be disk throughput")])])}),[],!1,null,null,null);t.default=s.exports}}]);