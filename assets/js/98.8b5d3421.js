(window.webpackJsonp=window.webpackJsonp||[]).push([[98],{475:function(e,t,o){"use strict";o.r(t);var n=o(45),s=Object(n.a)({},(function(){var e=this,t=e.$createElement,o=e._self._c||t;return o("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[o("h1",{attrs:{id:"_17-case-studies-relaxed-consistency-pnuts"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#_17-case-studies-relaxed-consistency-pnuts"}},[e._v("#")]),e._v(" 17.Case Studies: Relaxed Consistency-PNUTS")]),e._v(" "),o("p",[e._v("Cooper et al, PNUTS: Yahoo!'s Hosted Data Serving Platform. VLDB 2008.")]),e._v(" "),o("p",[e._v("why this paper?\nsame basic goals as Facebook/memcache and Spanner\nmulti-region data replication -- facing 100ms network delays\nmore principled design than Facebook/memcache, better semantics\nsimpler than Spanner\napps control trade-off between consistency and performance")]),e._v(" "),o("p",[e._v('PNUTS\' overall story similar to that of Spanner and Facebook/memcache\ndata centers ("regions") all over the world\neach region has web servers and storage\nthe big point: low latency via user contacting nearest region\nfor small records: user profile, messages, shopping cart, friend list, &c')]),e._v(" "),o("p",[e._v("design overview\n[diagram: 3 regions, browsers, apps, tablet ctlrs, routers, SUs, MBs]\neach region has all data\neach table partitioned by key over storage units\ntablet servers + routers know the partition plan")]),e._v(" "),o("p",[e._v("why replicas of "),o("em",[e._v("all")]),e._v(" data at multiple regions?\nany user can read anything quickly\ngood for data used by many users / many regions\ncross-region reads would be slow (~100ms)\nfailed SU repairable from another replica")]),e._v(" "),o("p",[e._v("drawbacks of a copy at each region?\nupdates need to be sent to every region\nlocal reads will probably be stale\nupdates from multiple regions need to be sorted out\nkeep replicas identical\ndon't lose updates (e.g. read-modify-write for counter)\nuses more disk space? not a problem for small records.")]),e._v(" "),o("p",[e._v("how do updates work?\napp server gets web request, needs to write data in PNUTS\nneed to update every region!\nwhy not just have app logic send update to every region?\nwhat if app crashes after updating only some regions?\nwhat if concurrent updates to same record?")]),e._v(" "),o("p",[e._v('PNUTS has a "record master" for each record\nmaster imposes order on writes for each record\nresponsible storage unit executes updates one at a time per record\ntells MB to broadcast updates in order to all regions')]),e._v(" "),o("p",[e._v("why per-record master?\neach record has a hidden column indicating region of record master\nfiner-grained than Facebook/memcache master region\nhopefully get best of both worlds:\nfast reads of local data\nfast writes b/c master is often the local region")]),e._v(" "),o("p",[e._v("the complete update story (some guesses):\napp wants to update some columns of a record, knows key")]),e._v(" "),o("ol",[o("li",[e._v("app sends update to local router")]),e._v(" "),o("li",[e._v("router forwards to local SU1")]),e._v(" "),o("li",[e._v("SU1 looks up record master for key: region R2")]),e._v(" "),o("li",[e._v("SU1 sends update request to router in R2")]),e._v(" "),o("li",[e._v("R2 router forwards update to local SU2")]),e._v(" "),o("li",[e._v("SU2 sends update to local Message Broker (MB)")]),e._v(" "),o("li",[e._v("MB stores on disk + backup MB, sends vers # to original app")]),e._v(" "),o("li",[e._v("MB sends update to MB at every region")]),e._v(" "),o("li",[e._v("every region updates local copy (MB, router, SU)")])]),e._v(" "),o("p",[e._v("MB is a neat idea\natomic: updates all replicas, or none\nrather than app server updating replicas (crash...)\nreliable: logs to disk, keeps trying, to cope with various failures\nordered: record's replicas eventually identical even w/ multiple writers\nasync: apps don't wait once write committed at MB")]),e._v(" "),o("p",[e._v("write order semantics\nsuppose each record in a table looks like:\nName   Where  What\nAlice  home   sleeping\nPNUTS preserves order of writes to a single record\ne.g. Alice does\nwrite(Alice.What, awake)\nwrite(Alice.Where, work)\nwrite RPC only returns after committed to record master's MB\nreaders may see home/sleeping or home/awake or work/awake\nno-one will see work / sleeping\nBUT writes to different records do not maintain order\ne.g.\nWrite(Bob.What, on duty)\nWrite(Alice.What, off duty)\nreaders may these in either order\ni.e. no-one on duty; or two people on duty\nif atomic hand-off important, need a single record with name of person")]),e._v(" "),o("p",[e._v('why is it OK for writes to take effect slowly?\nfundamental benefit: reads are then very fast, since local\nand reads usually greatly outnumber writes\nPNUTS mitigates write cost:\napplication waits for MB commit but not propagation ("asynchronous")\nmaster likely to be local (they claim 80% of the time)\nso MB commit will often be quick\nstill, eval says write takes 300ms if master is remote!\ndown side: readers at non-master regions may see stale data')]),e._v(" "),o("p",[e._v("how stale might a non-master record be?\ndepends on how quickly MB sends updates to regions\nguess: less than a second usually\nlonger if network slow/flaky, or MB busy")]),e._v(" "),o("p",[e._v("how do apps cope with potentially stale local replica?\nsometimes stale is ok: looking at other people's profiles\nsometimes stale is not ok: shopping cart after add/delete item\napplication gets to choose how consistent (section 2.2)\nread-any(k)\nread from local SU\nfast but maybe stale\nread-critical(k, required_version)\nfast local read if local SU has vers >= required_version\notherwise slow read from master SU\nwhy: app knows it just wrote, wants value to reflect write\nread-latest(k)\nalways read from master SU\nslow if master is remote!\nwhy: app needs fresh data")]),e._v(" "),o("p",[e._v("what if app needs to increment a counter stored in a record?\nis read-latest(k), increment, then write(k) enough?\nnot if there might be concurrent updates!")]),e._v(" "),o("p",[e._v("test-and-set-write(version#, new value) gives you atomic update to one record\nmaster rejects the write if current version # != version#\nso if concurrent updates, one will lose and retry\nwhile(1):\n(x, ver) = read-latest(k)\nif(t-a-s-w(k, ver, x+1))\nbreak\nt-a-s-w is fully general for single-record atomic read-modify-write")]),e._v(" "),o("p",[e._v("why not atomic multi-record writes?\ne.g. for bank transfers: Alice -= $10 ; Bob += $10\nwould that be easy to add to PNUTS?")]),e._v(" "),o("p",[e._v("The Question\nhow does PNUTS cope with Example 1 (page 2)\nInitially Alice's mother is in Alice's ACL, so mother can see photos")]),e._v(" "),o("ol",[o("li",[e._v("Alice removes her mother from ACL")]),e._v(" "),o("li",[e._v("Alice posts spring-break photos\ncould her mother see update #2 but not update #1?\nACL and photo list must be in the same record\nsince PNUTS guarantees order only for updates to same record\nHow could a storage system get this wrong?\nNo ordering through single master (e.g. Dynamo)")])]),e._v(" "),o("p",[e._v("What if Alice's mother does two reads:")]),e._v(" "),o("ol",[o("li",[e._v("reads the ACL")]),e._v(" "),o("li",[e._v("reads the photo list\n#1 could yield old ACL (with mother in it); #2 could yield new photo\nso: mother must read Alice's record just once")])]),e._v(" "),o("p",[e._v("how to change record's master if no failures?\ne.g. I move from Boston to LA\nperhaps just update the record, via old master?\nsince ID of master region is stored in the record\nold master announces change over MB\na few subsequent updates might go to the old master SU\nit will reject them, app retries and finds new master?")]),e._v(" "),o("p",[e._v("what about tolerating failures?")]),e._v(" "),o("p",[e._v("app server crashes midway through a set of updates\nnot a transaction, so only some of writes will happen\nbut master SU/MB either did or didn't get each write\nso each write happens at all regions, or none")]),e._v(" "),o("p",[e._v("SU down briefly, or network temporarily broken/lossy\n(I'm guessing here, could be wrong)\nMB keeps trying until SU acks\nSU shouldn't ACK until safely on disk")]),e._v(" "),o("p",[e._v("SU loses disk contents, or doesn't automatically reboot\nneed to restore disk content from SUs at other regions\n1. subscribe to MB feed, and save them for now\n2. copy content from SU at another region\n3. replay saved MB updates\npuzzle:\nhow to ensure we didn't miss any MB updates for this SU?\ne.g. subscribe to MB at time=100, but source SU only saw through 90?\nwill replay apply updates twice? is that harmful?\npaper mentions sending checkpoint message through MB\nmaybe fetch copy as of when the checkpoint arrived\nand only replay after the checkpoint\nBUT no ordering among MB streams from multiple regions")]),e._v(" "),o("p",[e._v("SU crashes in the middle of an update\ndoes SU update local disk, then send to MB?\nor does SU forward to MB, and only apply to disk after MB commits?")]),e._v(" "),o("p",[e._v("MB crashes after accepting update\nlogs to disks on two MB servers before ACKing\nMB recovery looks at log, (re)sends logged msgs\nrecord master SU maybe re-sends an update if MB crash before ACK\nmaybe record version #s will allow SUs to ignore duplicate")]),e._v(" "),o("p",[e._v("record's master region loses network connection\ncan other regions designate a replacement RM?\nno: original RM's MB may have logged updates, only some sent out\ndo other regions have to wait indefinitely? yes\nthis is one price of ordered updates / strict-ish consistency")]),e._v(" "),o("p",[e._v("now performance")]),e._v(" "),o("p",[e._v("evaluation focuses on latency, not throughput\nwhy are they apparently worried about latency but not throughput?\nmaybe throughput can be indefinitely increased by adding more SUs\nwhereas latency is harder to reduce")]),e._v(" "),o("p",[e._v("big performance question: why isn't the MB a terrible bottleneck?\nall writes funnel through a single pair of MBs\nwhich log every write to disk\ni do not know how they avoid this MB bottleneck")]),e._v(" "),o("p",[e._v("5.2: time for an insert while busy\nprobably measuring time until client sees commit reply from MB\ndepends on how far away Record Master is\nRM local: 75.6 ms\nRM nearby: 131.5 ms\nRM other coast: 315.5 ms")]),e._v(" "),o("p",[e._v("Why 75 ms for write to local RM?\nspeed-of-light delay?\nno: local\nqueuing, waiting for other client's operations?\nno: they imply 100 clients was max that didn't cause delay to rise\nEnd of 5.2 suggests 40 ms of 75 ms in in SU\nhow could it take 40 ms?\neach key/value is one file?\ncreating a file takes 3 disk writes (directory, inode, content)?\nwhat's the other 35 ms?\nMB disk write?")]),e._v(" "),o("p",[e._v("5.3 / Figure 3: effect of increasing request rate\nwhat do we expect for graph w/ x-axis req rate, y-axis latency?\nsystem has some inherent capacity, e.g. total disk seeks/second\nfor lower rates, constant latency\nfor higher rates, queue grows rapidly, avg latency blows up\nblow-up should be near max capacity of h/w\ne.g. # disk arms / seek time\nwe don't see a blow-up in Figure 3\nend of 5.3 says clients too slow\ntext says max possible rate was about 3000/second\n10% writes, so 300 writes/second\n5 SU per region, so 60 writes/SU/second\nabout right if each write does a random disk I/O\nbut you'll need lots of SUs for millions of active users\nmystery: how is MB able to log 300 writes/second?\nmaybe each region's MB only logs 1/3 of writes, i.e. 100 second?")]),e._v(" "),o("p",[e._v("stepping back, what were PNUTS key design decisions?")]),e._v(" "),o("ol",[o("li",[e._v("replication of all data at multiple regions\nfast reads, slow but async writes")]),e._v(" "),o("li",[e._v("relaxed consistency -- stale reads\nneeded if you want fast reads and async writes")]),e._v(" "),o("li",[e._v("sequence all writes thru record master\ngood for consistency; bad for latency")]),e._v(" "),o("li",[e._v("only single-row write semantics (order and t-a-s-w mini-transactions)\nsimplifies system; maybe awkward for applications")])]),e._v(" "),o("p",[e._v("Next: Dynamo, a very different design\nno master -- any region can update any value at any time\neventual consistency\ntree of versions if network partitions\nreaders must reconcile versions")])])}),[],!1,null,null,null);t.default=s.exports}}]);