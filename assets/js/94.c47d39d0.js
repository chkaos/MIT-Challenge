(window.webpackJsonp=window.webpackJsonp||[]).push([[94],{471:function(e,t,n){"use strict";n.r(t);var a=n(45),s=Object(a.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"_13-mapreduce-revisited"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_13-mapreduce-revisited"}},[e._v("#")]),e._v(" 13.MapReduce revisited")]),e._v(" "),n("p",[e._v("Why MapReduce?\nSecond look for fault tolerance and performance\nStarting point in current enthusiasm for big cluster computing\nA triumph of simplicity for programmer\nBulk orientation well matched to cluster with slow network\nVery influential, inspired many successors (Hadoop, Spark, &c)")]),e._v(" "),n("p",[e._v("Cluster computing for Big Data\n1000 computers + disks\na LAN\nsplit up data+computation among machines\ncommunicate as needed\nsimilar to DSM vision but much bigger, no desire for compatibilty")]),e._v(" "),n("p",[e._v("Example: inverted index\ne.g. index terabytes of web pages for a search engine\nInput:\nA collection of documents, e.g. crawled copy of entire web\ndoc 31: i am alex\ndoc 32: alex at 8 am\nOutput:\nalex: 31/3 32/1 ...\nam: 31/2 32/4 ...\nMap(document file i):\nsplit into words\nfor each offset j\nemit key=word[j] value=i/j\nReduce(word, list of d/o)\nemit word, sorted list of d/o")]),e._v(" "),n("p",[e._v("Diagram:")]),e._v(" "),n("ul",[n("li",[e._v("input partitioned into M splits on GFS: A, B, C, ...")]),e._v(" "),n("li",[e._v("Maps read local split, produce R local intermediate files (A0, A1 .. AR)")]),e._v(" "),n("li",[e._v("Reduce # = hash(key) % R")]),e._v(" "),n("li",[e._v("Reduce task i fetches Ai, Bi, Ci -- from every Map worker")]),e._v(" "),n("li",[e._v("Sort the fetched files to bring same key together")]),e._v(" "),n("li",[e._v("Call Reduce function on each key's values")]),e._v(" "),n("li",[e._v("Write output to GFS")]),e._v(" "),n("li",[e._v("Master controls all:\nMap task list\nReduce task list\nLocation of intermediate data (which Map worker ran which Map task)")])]),e._v(" "),n("p",[e._v("Notice:\nInput is huge -- terabytes\nInfo from all parts of input contributes to each output index entry\nSo terabytes must be communicated between machines\nOutput is huge -- terabytes")]),e._v(" "),n("p",[e._v("The main challenge: communication bottleneck\nThree kinds of data movement needed:\nRead huge input\nMove huge intermediate data\nStore huge output\nHow fast can one move data?\nRAM: 1000"),n("em",[e._v("1 GB/sec =    1000 GB/sec\ndisk: 1000")]),e._v("0.1 GB/sec =  100 GB/sec\nnet cross-section:        10 GB/sec\nExplain host link b/w vs net cross-section b/w")]),e._v(" "),n("p",[e._v("What are the crucial design decisions in MapReduce?\nContrast to DSM (TreadMarks or IVY, or even put/get in key/value store):\nThey allow arbitrary random interaction among threads.\nBut: latency sensitive, poor throughput efficiency.\nBut: recovering from crashed compute server very hard;\nprobably need global checkpoint, since states are\nintertwined.\nMaps and Reduces work on local data -> reduced network communication.\nFor Map, split storage and computation in the same way, use local disk.\nMaps and Reduces work on big batches of data -> no small latency-sensitive network messages.\nVery little interaction:\nMaps and Reduces can't interact with each other directly.\nNo interaction across phase boundaries.\n-> Can re-execute single Map/Reduce independently, no need for e.g. global checkpoint.\nProgrammer can't directly cause network communication,\nbut has indirect control since Map specifies key.")]),e._v(" "),n("p",[e._v("Where does MapReduce input come from?\nInput is striped+replicated over GFS in 64 MB chunks\nBut in fact Map always reads from a local disk\nThey run the Maps on the GFS server that holds the data\nTradeoff:\nGood: Map reads at disk speed, much faster than over net from GFS server\nBad: only two or three choices of where a given Map can run\npotential problem for load balance, stragglers")]),e._v(" "),n("p",[e._v("Where does MapReduce store intermediate data?\nOn the local disk of the Map server (not in GFS)\nTradeoff:\nGood: local disk write is faster than writing over network to GFS server\nBad: only one copy, potential problem for fault-tolerance and load-balance")]),e._v(" "),n("p",[e._v("Where does MapReduce store output?\nIn GFS, replicated, separate file per Reduce task\nSo output requires network communication -- slow\nThe reason: output can then be used as input for subsequent MapReduce")]),e._v(" "),n("p",[e._v("The Question: How soon after it receives the first file of\nintermediate data can a reduce worker start calling the application's\nReduce function?")]),e._v(" "),n("p",[e._v("Why does MapReduce postpone choice of which worker runs a Reduce?\nAfter all, might run faster if Map output directly streamed to reduce worker\nDynamic load balance!\nIf fixed in advance, one machine 2x slower -> 2x delay for whole computation\nand maybe the rest of the cluster idle/wasted half the time")]),e._v(" "),n("p",[e._v("Will MR scale?\nWill buying 2x machines yield 1/2 the run-time, indefinitely?\nMap calls probably scale\n2x machines -> each Map's input 1/2 as big -> done in 1/2 the time\nbut: input may not be infinitely partitionable\nbut: tiny input and intermediate files have high overhead\nReduce calls probably scale\n2x machines -> each handles 1/2 as many keys -> done in 1/2 the time\nbut: can't have more workers than keys\nbut: limited if some keys have more values than others\ne.g. \"the\" has vast number of values for inverted index\nso 2x machines -> no faster, since limited by key w/ most values\nNetwork may limit scaling, if large intermediate data\nMust spend money on faster core switches as well as more machines\nNot easy -- a hot R+D area now\nStragglers are a problem, if one machine is slow, or load imbalance\nCan't solve imbalance w/ more machines\nStart-up time is about a minute!!!\nCan't reduce w/ more machines (probably makes it worse)\nMore machines -> more failures")]),e._v(" "),n("p",[e._v("Now let's talk about fault tolerance\nThe challenge: paper says one server failure per job!\nToo frequent for whole-job restart to be attractive")]),e._v(" "),n("p",[e._v("The main idea: Map and Reduce are deterministic, functional, and independent,\nso MapReduce can deal with failures by re-executing\nOften a choice:\nRe-execute big tasks, or\nSave output, replicate, use small tasks\nBest tradeoff depends on frequency of failures and expense of communication")]),e._v(" "),n("p",[e._v("What if a worker fails while running Map?\nCan we restart just that Map on another machine?\nYes: GFS keeps copy of each input split on 3 machines\nMaster knows, tells Reduce workers where to find intermediate files")]),e._v(" "),n("p",[e._v("If a Map finishes, then that worker fails, do we need to re-run that Map?\nIntermediate output now inaccessible on worker's local disk.\nThus need to re-run Map elsewhere "),n("em",[e._v("unless")]),e._v(" all Reduce workers have\nalready fetched that Map's output.")]),e._v(" "),n("p",[e._v("What if Map had started to produce output, then crashed:\nWill some Reduces see Map's output twice?\nAnd thus produce e.g. word counts that are too high?")]),e._v(" "),n("p",[e._v("What if a worker fails while running Reduce?\nWhere can a replacement worker find Reduce input?\nIf a Reduce finishes, then worker fails, do we need to re-run?\nNo: Reduce output is stored+replicated in GFS.")]),e._v(" "),n("p",[e._v("Load balance\nWhat if some Map machines are faster than others?\nOr some input splits take longer to process?\nDon't want lots of idle machines and lots of work left to do!\nSolution: many more input splits than machines\nMaster hands out more Map tasks as machines finish\nThus faster machines do bigger share of work\nBut there's a constraint:\nWant to run Map task on machine that stores input data\nGFS keeps 3 replicas of each input data split\nSo only three efficient choices of where to run each Map task")]),e._v(" "),n("p",[e._v("Stragglers\nOften one machine is slow at finishing very last task\nh/w or s/w wedged, overloaded with some other work\nLoad balance only balances newly assigned tasks\nSolution: always schedule multiple copies of very last tasks!")]),e._v(" "),n("p",[e._v("How many Map/Reduce tasks vs workers should we have?\nThey use M = 10x number of workers, R = 2x.\nMore => finer grained load balance.\nMore => less redundant work for straggler reduction.\nMore => spread tasks of failed worker over more machines, re-execute faster.\nMore => overlap Map and shuffle, shuffle and Reduce.\nLess => big intermediate files w/ less overhead.\nM and R also maybe constrained by how data is striped in GFS.\ne.g. 64 MByte GFS chunks means M needs to total data size / 64 MBytes")]),e._v(" "),n("p",[e._v("Let's look at paper's performance evaluation")]),e._v(" "),n("p",[e._v("Figure 2 / Section 5.2\nText search for rare 3-char pattern, just Map, no shuffle or reduce\nOne terabyte of input\n1800 machines\nFigure 2 x-axis is time, y-axis is input read rate\n60 seconds start-up time "),n("em",[e._v("omitted")]),e._v("! (copying program, opening input files)\nWhy does it take so long (60 seconds) to reach the peak rate?\nWhy does it go up to 30,000 MB/s? Why not 3,000 or 300,000?\nThat's 17 MB/sec per server.\nWhat limits the peak rate?")]),e._v(" "),n("p",[e._v('Figure 3(a) / Section 5.3\nsorting a terabyte\nShould we be impressed by 800 seconds?\nTop graph -- Input rate\nWhy peak of 10,000 MB/s?\nWhy less than Figure 2\'s 30,000 MB/s? (writes disk)\nWhy does read phase last abt 100 seconds?\nMiddle graph -- Shuffle rate\nHow is shuffle able to start before Map phase finishes?\nmore map tasks than workers\nWhy does it peak at 5,000 MB/s?\nnet cross-sec b/w abt 18 GB/s\nWhy a gap, then starts again?\nruns some Reduce tasks, then fetches more\nWhy is the 2nd bump lower than first?\nmaybe competing w/ overlapped output writes\nLower graph -- Reduce output rate\nHow can reduces start before shuffle has finished?\nagain, shuffle gets all files for some tasks\nWhy is output rate so much lower than input rate?\nnet rather than disk; writes twice to GFS\nWhy the gap between apparent end of output and vertical "Done" line?\nstragglers?')]),e._v(" "),n("p",[e._v("What should we buy if we wanted sort to run faster?\nLet's guess how much each resource limits performance.\nReading input from disk: 30 GB/sec = 33 seconds (Figure 2)\nMap computation: between zero and 150 seconds (Figure 3(a) top)\nWriting intermediate to disk: ? (maybe 30 Gb/sec = 33 seconds)\nMap->Reduce across net: 5 GB/sec = 200 seconds\nLocal sort: 2*100 seconds (gap in Figure 3(a) middle)\nWriting output to GFS twice: 2.5 GB/sec = 400 seconds\nStragglers: 150 seconds? (Figure 3(a) bottom tail)\nThe answer: the network accounts for 600 of 850 seconds")]),e._v(" "),n("p",[e._v("Is it disappointing that sort uses only a small fraction of cluster CPU power?\nAfter all, only 200 of 800 seconds were spent sorting.\nAlternate view: they made good use of RAM and network.\nProbably critical that 1800 machines had more then a terabyte of RAM.\nAnd sorting is perhaps inherently about movement, not CPU.\nIf all they did was sort, they should sell CPUs/disks and buy a faster network.")]),e._v(" "),n("p",[e._v("Modern data centers have relatively faster networks\ne.g. FDS's 5.5 terabits/sec cross-section b/w vs MR paper's 150 gigabits/sec\nwhile CPUs are only modestly faster than in MR paper\nso today bottleneck might have shifted away from net, towards CPU")]),e._v(" "),n("p",[e._v("For what applications "),n("em",[e._v("doesn't")]),e._v(" MapReduce work well?\nSmall updates (re-run whole computation?)\nSmall unpredictable reads (neither Map nor Reduce can choose input)\nMultiple shuffles (can use multiple MR but not very efficient)\nIn general, data-flow graphs with more than two stages\nIteration (e.g. page-rank)")]),e._v(" "),n("p",[e._v("MapReduce retrospective\nSingle-handedly made big cluster computation popular\n(tho coincident w/ big datacenters, cheap servers, data-oriented companies)\nHadoop is still very popular\nInspired better successors (Spark, DryadLINQ, &c)")])])}),[],!1,null,null,null);t.default=s.exports}}]);