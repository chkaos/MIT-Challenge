(window.webpackJsonp=window.webpackJsonp||[]).push([[84],{461:function(e,t,a){"use strict";a.r(t);var n=a(45),i=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"_3-fault-tolerance-primary-backup-replication"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-fault-tolerance-primary-backup-replication"}},[e._v("#")]),e._v(" 3.Fault Tolerance: primary/backup replication")]),e._v(" "),a("p",[e._v("Today\nReplication\nRemus case study\nLab 2 introduction")]),e._v(" "),a("p",[e._v("Fault tolerance\nwe'd like a service that continues despite failures!\navailable: still useable despite [some class of] failures\ncorrect: act just like a single server to clients\nvery hard!\nvery useful!")]),e._v(" "),a("p",[e._v("Need a failure model: what will we try to cope with?\nIndependent fail-stop computer failure\nRemus further assumes only one failure at a time\nSite-wide power failure (and eventual reboot)\n(Network partition)\nNo bugs, no malice")]),e._v(" "),a("p",[e._v("Core idea: replication\n"),a("em",[e._v("Two")]),e._v(" servers (or more)\nEach replica keeps state needed for the service\nIf one replica fails, others can continue")]),e._v(" "),a("p",[e._v('Example: fault-tolerant MapReduce master\nlab 1 workers are already fault-tolerant, but not master\nmaster is a "single point of failure"\ncan we have two masters, in case one fails?\n[diagram: M1, M2, workers]\nstate:\nworker list\nwhich jobs done\nwhich workers idle\nTCP connection state\nprogram counter')]),e._v(" "),a("p",[e._v("Big Questions:\nWhat state to replicate?\nHow does replica get state?\nWhen to cut over to backup?\nAre anomalies visible at cut-over?\nHow to repair / re-integrate?")]),e._v(" "),a("p",[e._v('Two main approaches:\nState transfer\n"Primary" replica executes the service\nPrimary sends [new] state to backups\nReplicated state machine\nAll replicas execute all operations\nIf same start state,\nsame operations,\nsame order,\ndeterministic,\nthen same end state')]),e._v(" "),a("p",[e._v("State transfer is simpler\nBut state may be large, slow to transfer\nRemus uses state transfer")]),e._v(" "),a("p",[e._v("Replicated state machine can be more efficient\nIf operations are small compared to data\nBut complex, e.g. order on multi-core, determinism\nLabs use replicated state machines")]),e._v(" "),a("p",[e._v("Remus: High Availability via Asynchronous Virtual Machine Replication\nNSDI 2008")]),e._v(" "),a("p",[e._v("Very ambitious system:\nWhole-system replication\nCompletely transparent to applications and clients\nHigh availability for any existing software\nWould be magic if it worked well!\nFailure model:\n1. independent hardware faults\n2. site-wide power failure")]),e._v(" "),a("p",[e._v("Plan 1 (slow, broken):\n[diagram: app, O/S, Remus underneath]\ntwo machines, primary and backup; plus net and other machines\nprimary runs o/s and application s/w, talks to clients, &c\nbackup does "),a("em",[e._v("not")]),e._v(" initially execute o/s, applications, &c\nit only executes some Remus code\na few times per second:\npause primary\ncopy entire RAM, registers, disk to backup\nresume primary\nif primary fails:\nstart backup executing!")]),e._v(" "),a("p",[e._v("Q: is Plan 1 correct?\ni.e. does it look just like a single reliable server?")]),e._v(" "),a("p",[e._v("Q: what will outside world see if primary fails and replica takes over?\nwill backup have same state as last visible on primary?\nmight a client request be lost? executed twice?")]),e._v(" "),a("p",[e._v("Q: is Plan 1 efficient?")]),e._v(" "),a("p",[e._v("Can we eliminate the fact that backup "),a("em",[e._v("state")]),e._v(" trails the primary?\nSeems very hard!\nPrimary would have to tell backup (and wait) on every instruction.")]),e._v(" "),a("p",[e._v("Can we "),a("em",[e._v("conceal")]),e._v(" the fact that backup's state lags primary?\nPrevent outside world from "),a("em",[e._v("seeing")]),e._v(' that backup is behind last primary state\ne.g. prevent primary sent RPC reply but backup state doesn\'t reflect that RPC\ne.g. MR Register RPC, which it would be bad for backup to forget\nIdea: primary "holds" output until backup state catches up to output point\ne.g. primary receives RPC request, processes it, creates reply packet,\nbut Remus holds reply packet until backup has received corresponding state update')]),e._v(" "),a("p",[e._v("Remus epochs, checkpoints\nClients:    C1\nreq1                       reply1\nPrimary:    ... E1 ... | pause |  E2  release   | pause |\nckpt        ok           ckpt\nBackup:      ... (E0) ... |  apply  | (E1)         |")]),e._v(" "),a("ol",[a("li",[e._v("Primary runs for a while in Epoch 1, holding E1's output")]),e._v(" "),a("li",[e._v("Primary pauses")]),e._v(" "),a("li",[e._v("Primary sends RAM+disk changes to backup (in background)")]),e._v(" "),a("li",[e._v("Primary resumes execution in E2, holding E2's output")]),e._v(" "),a("li",[e._v("Backup copies all to separate RAM, then ACKs")]),e._v(" "),a("li",[e._v("Primary releases E1's output")]),e._v(" "),a("li",[e._v("Backup applies E1's changes to RAM and disk")])]),e._v(" "),a("p",[e._v("If primary fails, backup finishes applying last epoch's disk+RAM,\nthen starts executing")]),e._v(" "),a("p",[e._v("Q: any externally visible anomalies?\nlost input/output?\nrepeated output?")]),e._v(" "),a("p",[e._v("Q: what if primary receives+executes a request, crashes before checkpoint?\nbackup won't have seen request!")]),e._v(" "),a("p",[e._v("Q: what if primary crashes after sending ckpt to backup,\nbut before releasing output?")]),e._v(" "),a("p",[e._v("Q: what if client doesn't use TCP -- doesn't re-transmit?")]),e._v(" "),a("p",[e._v("Q: what if primary fails while sending state to backup?\ni.e. backup is mid-way through absorbing new state?")]),e._v(" "),a("p",[e._v("Q: are there situations in which Remus will incorrectly activate the backup?\ni.e. primary is actually alive\nnetwork partition...")]),e._v(" "),a("p",[e._v("Q: when primary recovers, how does Remus restore replication?\nneeded, since eventually active ex-backup will itself fail")]),e._v(" "),a("p",[e._v("Q: what if "),a("em",[e._v("both")]),e._v(" fail, e.g. site-wide power failure?\nRAM content will be lost, but disks will probably survive\nafter power is restored, reboot guest from one of the disks\nO/S and application recovery code will execute\ndisk must be \"crash-consistent\"\nso probably not the backup disk if was in middle of installing checkpoint\ndisk shouldn't reflect any held outputs (... why not?)\nso probably not the primary's disk if was executing\nI do not understand this part of the paper (Section 2.5)\nseems to be a window during which neither disk could be used if power failed\nprimary writes its disk during epoch\nmeanwhile backup applies last epoch's writes to its disk")]),e._v(" "),a("p",[e._v("Q: in what situations will Remus likely have good performance?")]),e._v(" "),a("p",[e._v("Q: in what situations will Remus likely have low performance?")]),e._v(" "),a("p",[e._v("Q: should epochs be short or long?")]),e._v(" "),a("p",[e._v("Remus Evaluation\nsummary: 1/2 to 1/4 native speed\ncheckpoints are big and take time to send\noutput hold limits speed at which clients can interact")]),e._v(" "),a("p",[e._v("Why so slow?\ncheckpoints are big and take time to generate and send\n100ms for SPECweb2005 -- because many pages written\nso inter-checkpoint intervals must be long\nso output must be held for quite a while\nso client interactions are slow\nonly 10 RPCs per second per client")]),e._v(" "),a("p",[e._v("How could one get better performance for replication?\nbig savings possible with application-specific schemes:\njust send state really needed by application, not all state\nsend state in optimized format, not whole pages\nsend operations if they are smaller than state\nlikely "),a("em",[e._v("not")]),e._v(" transaparent to applications\nand probably not to clients either")]),e._v(" "),a("p",[e._v("PRIMARY-BACKUP REPLICATION IN LAB 2")]),e._v(" "),a("p",[e._v("outline:\nsimple key/value database\nGet(k), Put(k, v), Append(k, v)\nprimary and backup\nreplicate by primary sending each operation to backups\ntolerate network problems, including partition\neither keep going, correctly\nor suspend operations until network is repaired\nallow replacement of failed servers\nyou implement essentially all of this (unlike lab 1)")]),e._v(" "),a("p",[e._v('"view server" decides who p and b are\nmain goal: avoid "split brain" -- disagreement about who primary is\nclients and servers ask view server\nthey don\'t make independent decisions')]),e._v(" "),a("p",[e._v('repair:\nview server can co-opt "idle" server as b after old b becomes p\nprimary initializes new backup\'s state')]),e._v(" "),a("p",[e._v("key points:")]),e._v(" "),a("ol",[a("li",[e._v("only one primary at a time!")]),e._v(" "),a("li",[e._v("the primary must have the latest state!\nwe will work out some rules to ensure these")])]),e._v(" "),a("p",[e._v('view server\nmaintains a sequence of "views"\nview #, primary, backup\n0: -- --\n1: S1 --\n2: S1 S2\n4: S2 --\n3: S2 S3\nmonitors server liveness\neach server periodically sends a Ping RPC\n"dead" if missed N Pings in a row\n"live" after single Ping\ncan be more than two servers Pinging view server\nif more than two, "idle" servers\nif primary is dead\nnew view with previous backup as primary\nif backup is dead, or no backup\nnew view with previously idle server as backup\nOK to have a view with just a primary, and no backup\nbut -- if an idle server is available, make it the backup')]),e._v(" "),a("p",[e._v("how to ensure new primary has up-to-date replica of state?\nonly promote previous backup\ni.e. don't make an idle server the primary\nbackup must remember if it has been initialized by primary\nif not, don't function as primary even if promoted!")]),e._v(" "),a("p",[e._v("Q: can more than one server think it is primary?\n1: S1, S2\nnet broken, so viewserver thinks S1 dead but it's alive\n2: S2, --\nnow S1 alive and not aware of view #2, so S1 still thinks it is primary\nAND S2 alive and thinks it is primary\n=> split brain, no good")]),e._v(" "),a("p",[e._v("how to ensure only one server acts as primary?\neven though more than one may "),a("em",[e._v("think")]),e._v(' it is primary\n"acts as" == executes and responds to client requests\nthe basic idea:\n1: S1 S2\n2: S2 --\nS1 still thinks it is primary\nS1 must forward ops to S2\nS2 thinks S2 is primary\nso S2 must reject S1\'s forwarded ops')]),e._v(" "),a("p",[e._v("the rules:")]),e._v(" "),a("ol",[a("li",[e._v("primary in view i must have been primary or backup in view i-1")]),e._v(" "),a("li",[e._v("if you think you are primary, must wait for backup for each request")]),e._v(" "),a("li",[e._v("if you think you are not backup, reject forwarded requests")]),e._v(" "),a("li",[e._v("if you think you are not primary, reject direct client requests")])]),e._v(" "),a("p",[e._v("so:\nbefore S2 hears about view #2\nS1 can process ops from clients, S2 will accept forwarded requests\nS2 will reject ops from clients who have heard about view #2\nafter S2 hears about view #2\nif S1 receives client request, it will forward, S2 will reject\nso S1 can no longer act as primary\nS1 will send error to client, client will ask vs for new view,\nclient will re-send to S2\nthe true moment of switch-over occurs when S2 hears about view #2")]),e._v(" "),a("p",[e._v("how can new backup get state?\ne.g. all the keys and values\nif S2 is backup in view i, but was not in view i-1,\nS2 should ask primary to transfer the complete state")]),e._v(" "),a("p",[e._v("rule for state transfer:\nevery operation (Put/Get/Append) must be either before or after state xfer\n== state xfer must be atomic w.r.t. operations\neither\nop is before, and xferred state reflects op\nop is after, xferred state doesn't reflect op, prim forwards op after state")]),e._v(" "),a("p",[e._v("Q: does primary need to forward Get()s to backup?\nafter all, Get() doesn't change anything, so why does backup need to know?\nand the extra RPC costs time")]),e._v(" "),a("p",[e._v("Q: how could we make primary-only Get()s work?")]),e._v(" "),a("p",[e._v("Q: are there cases when the lab 2 protocol cannot make forward progress?\nView service fails\nPrimary fails before new backup gets state\nWe will start fixing those in lab 3")])])}),[],!1,null,null,null);t.default=i.exports}}]);